{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lK64eFyp3nP0",
        "71JJQMdP4JFN",
        "KIjkreu2Hq3n"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJiOrDQt2C9y",
        "outputId": "41fa09f0-fcae-49d4-ad36-3b4733f7ec9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting BPEmb\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from BPEmb) (4.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from BPEmb) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from BPEmb) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from BPEmb) (2.27.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim->BPEmb) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim->BPEmb) (6.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->BPEmb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->BPEmb) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->BPEmb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->BPEmb) (3.4)\n",
            "Installing collected packages: sentencepiece, BPEmb\n",
            "Successfully installed BPEmb-0.3.4 sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "!pip install BPEmb\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from bpemb import BPEmb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll build a transformer from scratch, layer-by-layer. We'll start with the **Multi-Head Self-Attention** layer since that's the most involved bit. Once we have that working, the rest of the model will look familiar if you've been following the course so far."
      ],
      "metadata": {
        "id": "KT3OBWW33R1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Head Self-Attention"
      ],
      "metadata": {
        "id": "4pb_KTsr3kJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Scaled Dot Product Self-Attention\n",
        "\n",
        "\n",
        "Inside each attention head is a **Scaled Dot Product Self-Attention** operation as we covered in the slides. Given *queries*, *keys*, and *values*, the operation returns a new \"mix\" of the values.\n",
        "\n",
        "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
        "\n",
        "The following function implements this and also takes a mask to account for padding and for masking future tokens for decoding (i.e. **look-ahead mask**)."
      ],
      "metadata": {
        "id": "lK64eFyp3nP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "  key_dim = tf.cast(tf.shape(key)[-1], tf.float32)  # dk\n",
        "  scaled_scores = tf.matmul(query, key, transpose_b=True) / np.sqrt(key_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
        "\n",
        "  softmax = tf.keras.layers.Softmax()\n",
        "  weights = softmax(scaled_scores)\n",
        "  return tf.matmul(weights, value), weights"
      ],
      "metadata": {
        "id": "SIKDcKrc2FTH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose our *queries*, *keys*, and *values* are each a length of 3 with a dimension of 4.\n",
        "seq_len = 3\n",
        "embed_dim = 4\n",
        "\n",
        "queries = np.random.rand(seq_len, embed_dim)\n",
        "keys    = np.random.rand(seq_len, embed_dim)\n",
        "values  = np.random.rand(seq_len, embed_dim)\n",
        "\n",
        "print(\"Queries:\\n\", queries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSEEbjpg5GVA",
        "outputId": "18b97954-44c8-4efc-99fa-4ad00a62a94d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries:\n",
            " [[0.43176425 0.15350907 0.78373318 0.66364481]\n",
            " [0.13514988 0.79738097 0.0792238  0.48807841]\n",
            " [0.69380604 0.0067362  0.92388612 0.56572229]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# self-attention output and weights\n",
        "output, attn_weights = scaled_dot_product_attention(queries, keys, values)\n",
        "\n",
        "print(\"Output\\n\", output, \"\\n\")\n",
        "print(\"Weights\\n\", attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7FNgzPg5emT",
        "outputId": "5b1f02a8-0c20-4be6-c94c-98fe2e870756"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output\n",
            " tf.Tensor(\n",
            "[[0.48552772 0.32565042 0.4158109  0.39738902]\n",
            " [0.52122736 0.27932334 0.40136355 0.38737875]\n",
            " [0.4752882  0.34118706 0.41985312 0.3984257 ]], shape=(3, 4), dtype=float32) \n",
            "\n",
            "Weights\n",
            " tf.Tensor(\n",
            "[[0.34662864 0.3022894  0.3510819 ]\n",
            " [0.2747659  0.37801412 0.34721997]\n",
            " [0.37168592 0.2791511  0.34916294]], shape=(3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating queries, keys, and values for multiple heads.\n",
        "\n",
        "Now that we have a way to calculate self-attention, let's actually generate the input *queries*, *keys*, and *values* for multiple heads."
      ],
      "metadata": {
        "id": "71JJQMdP4JFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the slides (and in most references), each attention head had its <u>own separate</u> set of *query*, *key*, and *value* weights. Each weight matrix was of dimension $d\\ x \\ d/h$ where h was the number of heads. \n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1SLWkHQgy4nQPFvvjG5_V8UTtpSAJ2zrr)"
      ],
      "metadata": {
        "id": "TGwMoL106KNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's easier to understand things this way and we can certainly code it this way as well. But we can also \"simulate\" different heads with a single query matrix, single key matrix, and single value matrix.\n",
        "<br><br>\n",
        "We'll do both. First we'll create *query*, *key*, and *value* vectors using separate weights per head.\n"
      ],
      "metadata": {
        "id": "aF7UIgBe6OpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# an example of 12 dimensional embeddings processed by three attentions heads\n",
        "batch_size = 1\n",
        "seq_len = 3\n",
        "embed_dim = 12\n",
        "num_heads = 3\n",
        "head_dim = embed_dim // num_heads\n",
        "\n",
        "print(f\"Dimension of each head: {head_dim}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1RPO1bV5GSh",
        "outputId": "f36fc58a-ed46-422a-b8c0-af85187ad6f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension of each head: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using separate weight matrices per head\n",
        "\n",
        "Suppose these are our input embeddings. Here we have a batch of 1 containing a sequence of length 3, with each element being a 12-dimensional embedding."
      ],
      "metadata": {
        "id": "pHCIYEvZ62UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.rand(batch_size, seq_len, embed_dim).round(1)\n",
        "print(\"Input shape: \", x.shape, \"\\n\")\n",
        "print(\"Input:\\n\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LAbofZ66XFg",
        "outputId": "7ed0a57a-30a2-4201-bcc3-88ab95b5bb86"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:  (1, 3, 12) \n",
            "\n",
            "Input:\n",
            " [[[0.9 0.1 0.9 0.1 0.3 0.9 0.2 0.7 0.8 0.  0.2 0.5]\n",
            "  [0.1 0.  0.8 0.1 0.  0.8 0.9 0.5 0.  0.6 0.1 0.7]\n",
            "  [0.1 0.6 0.1 0.5 0.6 0.2 0.3 0.4 0.6 0.7 0.8 1. ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll declare three sets of *query* weights (one for each head), three sets of *key* weights, and three sets of *value* weights. Remember each weight matrix should have a dimension of $\\text{d}\\ \\text{x}\\ \\text{d/h}$."
      ],
      "metadata": {
        "id": "c_AdPlYU7c8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The query weights for each head\n",
        "wq0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wq1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wq2 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "\n",
        "# The key weights for each head\n",
        "wk0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wk1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wk2 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "\n",
        "# The value weights for each head\n",
        "wv0 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wv1 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "wv2 = np.random.rand(embed_dim, head_dim).round(1)\n",
        "\n",
        "print(\"The three sets of query weights (one for each head):\")\n",
        "print(\"wq0:\\n\", wq0)\n",
        "print(\"wq1:\\n\", wq1)\n",
        "print(\"wq2:\\n\", wq1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdGX7Tze7ROn",
        "outputId": "ac049cb7-3c8f-4f0e-f80a-398ffb2f5999"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The three sets of query weights (one for each head):\n",
            "wq0:\n",
            " [[0.1 0.7 0.7 0.5]\n",
            " [0.9 0.2 0.6 0.8]\n",
            " [0.5 0.8 0.7 0.9]\n",
            " [0.7 0.9 0.5 0.3]\n",
            " [0.9 0.9 0.7 0.2]\n",
            " [0.5 0.9 0.9 0.8]\n",
            " [0.7 0.7 0.4 0.9]\n",
            " [0.4 0.  0.3 0.8]\n",
            " [0.6 0.6 0.4 0.7]\n",
            " [0.1 0.6 0.4 0.2]\n",
            " [0.4 0.6 0.1 0.3]\n",
            " [0.8 0.9 0.5 0.6]]\n",
            "wq1:\n",
            " [[0.9 0.7 0.9 0.3]\n",
            " [0.3 0.1 0.4 1. ]\n",
            " [0.7 0.3 0.5 0.6]\n",
            " [0.6 0.2 0.4 0.8]\n",
            " [0.  0.  0.2 0.6]\n",
            " [1.  0.5 0.9 0.2]\n",
            " [0.7 0.5 0.7 0.3]\n",
            " [0.7 0.3 1.  0.4]\n",
            " [0.4 0.6 0.9 1. ]\n",
            " [0.1 0.2 0.  0.1]\n",
            " [0.  0.2 0.6 0.6]\n",
            " [0.7 0.7 0.9 0.8]]\n",
            "wq2:\n",
            " [[0.9 0.7 0.9 0.3]\n",
            " [0.3 0.1 0.4 1. ]\n",
            " [0.7 0.3 0.5 0.6]\n",
            " [0.6 0.2 0.4 0.8]\n",
            " [0.  0.  0.2 0.6]\n",
            " [1.  0.5 0.9 0.2]\n",
            " [0.7 0.5 0.7 0.3]\n",
            " [0.7 0.3 1.  0.4]\n",
            " [0.4 0.6 0.9 1. ]\n",
            " [0.1 0.2 0.  0.1]\n",
            " [0.  0.2 0.6 0.6]\n",
            " [0.7 0.7 0.9 0.8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll generate our *queries*, *keys*, and *values* for each head by multiplying our input by the weights."
      ],
      "metadata": {
        "id": "Qycxj7iX8Wwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Geneated queries, keys, and values for the first head\n",
        "q0 = np.dot(x, wq0)\n",
        "k0 = np.dot(x, wk0)\n",
        "v0 = np.dot(x, wv0)\n",
        "\n",
        "# Geneated queries, keys, and values for the second head\n",
        "q1 = np.dot(x, wq1)\n",
        "k1 = np.dot(x, wk1)\n",
        "v1 = np.dot(x, wv1)\n",
        "\n",
        "# Geneated queries, keys, and values for the second head\n",
        "q2 = np.dot(x, wq2)\n",
        "k2 = np.dot(x, wk2)\n",
        "v2 = np.dot(x, wv2)\n",
        "\n",
        "print(\"Q, K, and V for first head:\\n\")\n",
        "\n",
        "print(f\"q0 {q0.shape}:\\n\", q0, \"\\n\")\n",
        "print(f\"k0 {k0.shape}:\\n\", k0, \"\\n\")\n",
        "print(f\"v0 {v0.shape}:\\n\", v0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQlrHghE8bKW",
        "outputId": "6f245558-51c8-4f09-81d0-df4f2ef8b091"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q, K, and V for first head:\n",
            "\n",
            "q0 (1, 3, 4):\n",
            " [[[3.67 4.38 3.06 3.6 ]\n",
            "  [3.41 4.08 3.14 3.71]\n",
            "  [3.01 3.58 2.93 3.  ]]] \n",
            "\n",
            "k0 (1, 3, 4):\n",
            " [[[3.59 3.33 2.19 3.24]\n",
            "  [3.82 3.57 2.27 3.32]\n",
            "  [3.13 3.07 2.12 3.26]]] \n",
            "\n",
            "v0 (1, 3, 4):\n",
            " [[[2.54 4.   3.93 3.58]\n",
            "  [2.92 3.83 3.23 3.8 ]\n",
            "  [2.85 3.4  3.5  3.37]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our Q, K, V vectors, we can just pass them to our self-attention operation. Here we're calculating the output and attention weights for the first head."
      ],
      "metadata": {
        "id": "bPKzX9Kx9rwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out0, attn_weights0 = scaled_dot_product_attention(q0, k0, v0)\n",
        "\n",
        "print(\"Output from first attention head: \", out0, \"\\n\")\n",
        "print(\"Attention weights from first head: \", attn_weights0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FARiEdDm8N93",
        "outputId": "2e921542-f7db-4a02-ad73-d27759a620d5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from first attention head:  tf.Tensor(\n",
            "[[[2.833825  3.8457968 3.3957014 3.7308974]\n",
            "  [2.8301964 3.8441498 3.4033847 3.7260363]\n",
            "  [2.8210227 3.8409605 3.422514  3.7145672]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "Attention weights from first head:  tf.Tensor(\n",
            "[[[0.21769002 0.73298293 0.04932702]\n",
            "  [0.22593231 0.7176527  0.05641498]\n",
            "  [0.24716169 0.6806128  0.07222551]]], shape=(1, 3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are the other two (attention weights are ignored)\n",
        "out1, _ = scaled_dot_product_attention(q1, k1, v1)\n",
        "out2, _ = scaled_dot_product_attention(q2, k2, v2)\n",
        "\n",
        "print(\"Output from second attention head: \", out1, \"\\n\")\n",
        "print(\"Output from third attention head: \", out2,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaJaR5AJ9324",
        "outputId": "26c51484-6a68-4874-a6a9-78f7aab77393"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from second attention head:  tf.Tensor(\n",
            "[[[2.4284098 2.32754   3.0384266 4.010263 ]\n",
            "  [2.441395  2.312757  3.039845  4.000343 ]\n",
            "  [2.4330301 2.3443832 3.0244172 4.004699 ]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "Output from third attention head:  tf.Tensor(\n",
            "[[[3.0552158 1.5740515 3.0670934 3.2499583]\n",
            "  [3.0592449 1.5751076 3.068358  3.2518098]\n",
            "  [3.036745  1.5701491 3.061039  3.2413504]]], shape=(1, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# once we have each head's output, \n",
        "# we concatenate them and then put them through a linear layer for further processing\n",
        "combined_out_a = np.concatenate((out0, out1, out2), axis=-1)\n",
        "print(f\"Combined output from all heads {combined_out_a.shape}:\")\n",
        "print(combined_out_a)\n",
        "\n",
        "# The final step would be to run combined_out_a through a linear/dense layer \n",
        "# for further processing."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G1jT_h19-gW",
        "outputId": "408e6cd2-c394-49c4-d537-4fe673ecfb76"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined output from all heads (1, 3, 12):\n",
            "[[[2.833825  3.8457968 3.3957014 3.7308974 2.4284098 2.32754   3.0384266\n",
            "   4.010263  3.0552158 1.5740515 3.0670934 3.2499583]\n",
            "  [2.8301964 3.8441498 3.4033847 3.7260363 2.441395  2.312757  3.039845\n",
            "   4.000343  3.0592449 1.5751076 3.068358  3.2518098]\n",
            "  [2.8210227 3.8409605 3.422514  3.7145672 2.4330301 2.3443832 3.0244172\n",
            "   4.004699  3.036745  1.5701491 3.061039  3.2413504]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So that's a complete run of **multi-head self-attention** using separate sets of weights per head.<br>\n",
        "\n",
        "### Using a single query weight matrix, single key weight matrix, and single value weight matrix.\n",
        "\n",
        "These were our separate per-head query weights:"
      ],
      "metadata": {
        "id": "eHjbM1-U_wwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Query weights for first head: \\n\", wq0, \"\\n\")\n",
        "print(\"Query weights for second head: \\n\", wq1, \"\\n\")\n",
        "print(\"Query weights for third head: \\n\", wq2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_PQCLcs_xEV",
        "outputId": "07d40082-68ca-48f0-f39b-0058e0e279ee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query weights for first head: \n",
            " [[0.1 0.7 0.7 0.5]\n",
            " [0.9 0.2 0.6 0.8]\n",
            " [0.5 0.8 0.7 0.9]\n",
            " [0.7 0.9 0.5 0.3]\n",
            " [0.9 0.9 0.7 0.2]\n",
            " [0.5 0.9 0.9 0.8]\n",
            " [0.7 0.7 0.4 0.9]\n",
            " [0.4 0.  0.3 0.8]\n",
            " [0.6 0.6 0.4 0.7]\n",
            " [0.1 0.6 0.4 0.2]\n",
            " [0.4 0.6 0.1 0.3]\n",
            " [0.8 0.9 0.5 0.6]] \n",
            "\n",
            "Query weights for second head: \n",
            " [[0.9 0.7 0.9 0.3]\n",
            " [0.3 0.1 0.4 1. ]\n",
            " [0.7 0.3 0.5 0.6]\n",
            " [0.6 0.2 0.4 0.8]\n",
            " [0.  0.  0.2 0.6]\n",
            " [1.  0.5 0.9 0.2]\n",
            " [0.7 0.5 0.7 0.3]\n",
            " [0.7 0.3 1.  0.4]\n",
            " [0.4 0.6 0.9 1. ]\n",
            " [0.1 0.2 0.  0.1]\n",
            " [0.  0.2 0.6 0.6]\n",
            " [0.7 0.7 0.9 0.8]] \n",
            "\n",
            "Query weights for third head: \n",
            " [[0.4 0.1 0.6 0.8]\n",
            " [0.7 0.9 0.5 0.6]\n",
            " [0.6 0.9 0.6 0.2]\n",
            " [0.5 0.8 1.  0.8]\n",
            " [0.8 0.6 0.7 0. ]\n",
            " [0.8 0.3 0.3 0.6]\n",
            " [0.6 0.  0.7 0.9]\n",
            " [0.5 0.9 0.7 0.6]\n",
            " [0.5 0.4 0.1 0.3]\n",
            " [0.5 0.9 0.6 0.8]\n",
            " [0.5 0.3 0.4 0.4]\n",
            " [0.6 0.7 0.8 0. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose instead of declaring three separate query weight matrices, we had declared one. i.e. a single $d\\ x\\ d$ matrix. We're concatenating our per-head query weights here instead of declaring a new set of weights so that we get the same results."
      ],
      "metadata": {
        "id": "SM-L7V-8AI3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wq = np.concatenate((wq0, wq1, wq2), axis=1)\n",
        "print(f\"Single query weight matrix {wq.shape}: \\n\", wq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E81qYIS3AGjM",
        "outputId": "7142476e-6f33-4cd3-fcc8-131cd52d2aba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single query weight matrix (12, 12): \n",
            " [[0.1 0.7 0.7 0.5 0.9 0.7 0.9 0.3 0.4 0.1 0.6 0.8]\n",
            " [0.9 0.2 0.6 0.8 0.3 0.1 0.4 1.  0.7 0.9 0.5 0.6]\n",
            " [0.5 0.8 0.7 0.9 0.7 0.3 0.5 0.6 0.6 0.9 0.6 0.2]\n",
            " [0.7 0.9 0.5 0.3 0.6 0.2 0.4 0.8 0.5 0.8 1.  0.8]\n",
            " [0.9 0.9 0.7 0.2 0.  0.  0.2 0.6 0.8 0.6 0.7 0. ]\n",
            " [0.5 0.9 0.9 0.8 1.  0.5 0.9 0.2 0.8 0.3 0.3 0.6]\n",
            " [0.7 0.7 0.4 0.9 0.7 0.5 0.7 0.3 0.6 0.  0.7 0.9]\n",
            " [0.4 0.  0.3 0.8 0.7 0.3 1.  0.4 0.5 0.9 0.7 0.6]\n",
            " [0.6 0.6 0.4 0.7 0.4 0.6 0.9 1.  0.5 0.4 0.1 0.3]\n",
            " [0.1 0.6 0.4 0.2 0.1 0.2 0.  0.1 0.5 0.9 0.6 0.8]\n",
            " [0.4 0.6 0.1 0.3 0.  0.2 0.6 0.6 0.5 0.3 0.4 0.4]\n",
            " [0.8 0.9 0.5 0.6 0.7 0.7 0.9 0.8 0.6 0.7 0.8 0. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In the same vein, pretend we declared a single key weight matrix, and single value weight matrix.\n",
        "wk = np.concatenate((wk0, wk1, wk2), axis=1)\n",
        "wv = np.concatenate((wv0, wv1, wv2), axis=1)\n",
        "\n",
        "print(f\"Single key weight matrix {wk.shape}:\\n\", wk, \"\\n\")\n",
        "print(f\"Single value weight matrix {wv.shape}:\\n\", wv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOJubdTzAGgz",
        "outputId": "8db0f1d6-af4e-4432-85ba-bb47aa2136d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single key weight matrix (12, 12):\n",
            " [[0.5 0.4 0.5 0.9 1.  0.6 0.2 0.5 0.9 0.8 0.8 0.4]\n",
            " [0.9 0.9 0.  0.7 0.9 0.7 0.4 0.4 0.7 0.9 0.2 0.7]\n",
            " [0.6 0.8 0.7 0.7 0.9 0.4 0.7 0.3 0.  0.9 0.2 0.5]\n",
            " [0.9 0.4 0.5 1.  0.2 0.6 0.8 0.4 0.9 0.5 0.8 0.8]\n",
            " [0.5 0.3 0.2 0.9 0.3 0.9 0.5 1.  0.3 0.5 0.1 0. ]\n",
            " [0.9 0.6 0.5 0.4 0.7 0.1 0.8 0.5 0.8 0.1 0.7 0.2]\n",
            " [0.4 0.8 0.1 1.  0.4 0.6 0.1 0.8 0.5 0.5 0.8 0.3]\n",
            " [0.1 0.7 0.9 0.5 0.7 0.  0.8 1.  0.3 0.8 0.2 0.2]\n",
            " [0.7 0.5 0.2 0.1 0.5 0.9 0.7 1.  0.5 0.3 0.  0.9]\n",
            " [0.3 0.5 0.4 0.7 0.8 0.6 0.  0.8 0.1 0.7 0.1 0.5]\n",
            " [0.6 0.6 0.8 0.1 0.9 0.1 0.6 0.9 0.  0.4 0.9 0.3]\n",
            " [0.9 0.4 0.2 0.  0.9 1.  0.7 0.  0.8 0.6 0.6 0.1]] \n",
            "\n",
            "Single value weight matrix (12, 12):\n",
            " [[0.9 0.2 0.3 0.9 0.7 0.3 0.5 0.6 0.4 1.  0.8 0.9]\n",
            " [0.9 0.7 0.4 0.2 0.3 0.5 0.  0.8 0.4 0.6 0.6 0.5]\n",
            " [0.6 0.5 0.8 0.7 0.1 0.7 1.  0.8 0.2 0.  0.  0.4]\n",
            " [0.2 1.  0.9 0.6 0.4 0.1 0.7 0.  0.3 0.1 0.7 0. ]\n",
            " [0.3 0.4 0.9 0.8 0.4 0.7 0.5 0.8 0.2 0.5 0.6 0. ]\n",
            " [0.2 0.  0.3 0.8 0.9 0.2 0.3 1.  0.6 0.1 0.5 0.2]\n",
            " [0.2 0.8 0.6 0.5 0.3 0.1 0.5 1.  0.6 0.3 0.4 0.9]\n",
            " [0.  0.6 0.  0.8 0.6 0.1 0.7 0.5 0.1 0.2 0.3 0.9]\n",
            " [0.4 1.  0.6 0.3 0.7 0.2 0.1 0.8 0.3 0.2 0.8 0.9]\n",
            " [0.7 0.9 0.6 0.9 0.7 0.9 0.9 0.7 0.7 0.5 0.3 0.8]\n",
            " [0.7 0.8 1.  0.4 0.3 0.6 0.3 0.6 0.9 0.1 0.5 0.3]\n",
            " [0.8 0.8 0.5 0.6 0.1 0.2 0.5 0.1 0.6 0.1 0.8 0.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate all our queries, keys, and values with three dot products\n",
        "q_s = np.dot(x, wq)\n",
        "k_s = np.dot(x, wk)\n",
        "v_s = np.dot(x, wv)\n",
        "\n",
        "print(f\"Query vectors using a single weight matrix {q_s.shape}:\\n\", q_s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCEgb-N-AGek",
        "outputId": "7a5ade82-9d8a-428d-8df2-a88d44f58882"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query vectors using a single weight matrix (1, 3, 12):\n",
            " [[[3.67 4.38 3.06 3.6  2.86 2.21 3.62 3.48 3.59 3.13 3.35 2.26]\n",
            "  [3.41 4.08 3.14 3.71 3.4  2.36 3.8  3.   3.63 3.3  3.66 3.18]\n",
            "  [3.01 3.58 2.93 3.   2.13 1.63 2.82 3.2  3.24 3.27 2.85 2.18]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Somehow, we need to separate these vectors such they're treated like three separate sets by the self-attention operation."
      ],
      "metadata": {
        "id": "Z4rVTdgnA3X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(q0, \"\\n\")\n",
        "print(q1, \"\\n\")\n",
        "print(q2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRun1kh9Az6h",
        "outputId": "84b194b6-107f-477d-debf-7313a23bd6d6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[3.67 4.38 3.06 3.6 ]\n",
            "  [3.41 4.08 3.14 3.71]\n",
            "  [3.01 3.58 2.93 3.  ]]] \n",
            "\n",
            "[[[2.86 2.21 3.62 3.48]\n",
            "  [3.4  2.36 3.8  3.  ]\n",
            "  [2.13 1.63 2.82 3.2 ]]] \n",
            "\n",
            "[[[3.59 3.13 3.35 2.26]\n",
            "  [3.63 3.3  3.66 3.18]\n",
            "  [3.24 3.27 2.85 2.18]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how each set of per-head queries looks like we took the combined queries, and chopped them vertically every four dimensions.\n",
        "<br><br>\n",
        "We can split our combined queries into $\\text{d}\\ \\text{x}\\ \\text{d/h}$ heads using **reshape** and **transpose**.<br><br>\n",
        "The first step is to *reshape* our combined queries from a shape of:<br>\n",
        "(batch_size, seq_len, embed_dim)<br>\n",
        "\n",
        "into a shape of<br>\n",
        " (batch_size, seq_len, num_heads, head_dim).\n",
        " <br>"
      ],
      "metadata": {
        "id": "0B3ogszUA7dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: we can achieve the same thing by passing -1 instead of seq_len\n",
        "q_s_reshaped = tf.reshape(q_s, (batch_size, seq_len, num_heads, head_dim))\n",
        "\n",
        "print(f\"Combined queries: {q_s.shape}\\n\", q_s, \"\\n\")\n",
        "print(f\"Reshaped into separate heads: {q_s_reshaped.shape}\\n\", q_s_reshaped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBmZ4RtIA9fG",
        "outputId": "bdef91aa-87bd-4833-f80c-bf0f1f42bb0e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined queries: (1, 3, 12)\n",
            " [[[3.67 4.38 3.06 3.6  2.86 2.21 3.62 3.48 3.59 3.13 3.35 2.26]\n",
            "  [3.41 4.08 3.14 3.71 3.4  2.36 3.8  3.   3.63 3.3  3.66 3.18]\n",
            "  [3.01 3.58 2.93 3.   2.13 1.63 2.82 3.2  3.24 3.27 2.85 2.18]]] \n",
            "\n",
            "Reshaped into separate heads: (1, 3, 3, 4)\n",
            " tf.Tensor(\n",
            "[[[[3.67 4.38 3.06 3.6 ]\n",
            "   [2.86 2.21 3.62 3.48]\n",
            "   [3.59 3.13 3.35 2.26]]\n",
            "\n",
            "  [[3.41 4.08 3.14 3.71]\n",
            "   [3.4  2.36 3.8  3.  ]\n",
            "   [3.63 3.3  3.66 3.18]]\n",
            "\n",
            "  [[3.01 3.58 2.93 3.  ]\n",
            "   [2.13 1.63 2.82 3.2 ]\n",
            "   [3.24 3.27 2.85 2.18]]]], shape=(1, 3, 3, 4), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we have our desired shape. The next step is to *transpose* it such that simulates vertically chopping our combined queries. By transposing, our matrix dimensions become:<br>\n",
        "(batch_size, num_heads, seq_len, head_dim)<br>"
      ],
      "metadata": {
        "id": "oohF002-BnZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_s_transposed = tf.transpose(q_s_reshaped, perm=[0, 2, 1, 3]).numpy()\n",
        "\n",
        "print(f\"Queries transposed into \\\"separate\\\" heads {q_s_transposed.shape}:\\n\", \n",
        "      q_s_transposed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqkPNlxyBWPX",
        "outputId": "365d5eaf-03ce-46fd-96b1-f2a00f9a15d4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queries transposed into \"separate\" heads (1, 3, 3, 4):\n",
            " [[[[3.67 4.38 3.06 3.6 ]\n",
            "   [3.41 4.08 3.14 3.71]\n",
            "   [3.01 3.58 2.93 3.  ]]\n",
            "\n",
            "  [[2.86 2.21 3.62 3.48]\n",
            "   [3.4  2.36 3.8  3.  ]\n",
            "   [2.13 1.63 2.82 3.2 ]]\n",
            "\n",
            "  [[3.59 3.13 3.35 2.26]\n",
            "   [3.63 3.3  3.66 3.18]\n",
            "   [3.24 3.27 2.85 2.18]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we compare this against the separate per-head queries we calculated previously, we see the same result except we now have all our queries in a single matrix."
      ],
      "metadata": {
        "id": "LR2c7z4OB6gC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The separate per-head query matrices from before: \")\n",
        "print(q0, \"\\n\")\n",
        "print(q1, \"\\n\")\n",
        "print(q2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kFucIatB5Nn",
        "outputId": "467bf3f1-439c-4436-9759-f20ac7d03651"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The separate per-head query matrices from before: \n",
            "[[[3.67 4.38 3.06 3.6 ]\n",
            "  [3.41 4.08 3.14 3.71]\n",
            "  [3.01 3.58 2.93 3.  ]]] \n",
            "\n",
            "[[[2.86 2.21 3.62 3.48]\n",
            "  [3.4  2.36 3.8  3.  ]\n",
            "  [2.13 1.63 2.82 3.2 ]]] \n",
            "\n",
            "[[[3.59 3.13 3.35 2.26]\n",
            "  [3.63 3.3  3.66 3.18]\n",
            "  [3.24 3.27 2.85 2.18]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do the exact same thing with our combined keys and values."
      ],
      "metadata": {
        "id": "y0r4PBm9CINb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_s_transposed = tf.transpose(tf.reshape(k_s, (batch_size, seq_len, num_heads, head_dim)), perm=[0,2,1,3]).numpy()\n",
        "v_s_transposed = tf.transpose(tf.reshape(v_s, (batch_size, seq_len, num_heads, head_dim)), perm=[0,2,1,3]).numpy()\n",
        "\n",
        "print(f\"Keys for all heads in a single matrix {k_s.shape}: \\n\", k_s_transposed, \"\\n\")\n",
        "print(f\"Values for all heads in a single matrix {v_s.shape}: \\n\", v_s_transposed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFDJAe5nDGLU",
        "outputId": "61aa22e8-9d7e-4de0-ac37-e3b463f5e501"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys for all heads in a single matrix (1, 3, 12): \n",
            " [[[[3.59 3.33 2.19 3.24]\n",
            "   [3.82 3.57 2.27 3.32]\n",
            "   [3.13 3.07 2.12 3.26]]\n",
            "\n",
            "  [[3.6  3.66 3.25 3.91]\n",
            "   [4.2  3.19 3.01 3.34]\n",
            "   [3.67 3.27 2.7  3.81]]\n",
            "\n",
            "  [[2.41 3.12 2.36 2.23]\n",
            "   [3.16 3.32 3.12 2.09]\n",
            "   [1.96 3.29 1.6  2.33]]]] \n",
            "\n",
            "Values for all heads in a single matrix (1, 3, 12): \n",
            " [[[[2.54 4.   3.93 3.58]\n",
            "   [2.92 3.83 3.23 3.8 ]\n",
            "   [2.85 3.4  3.5  3.37]]\n",
            "\n",
            "  [[2.35 2.31 3.1  4.08]\n",
            "   [2.66 2.1  3.04 3.83]\n",
            "   [2.43 2.75 2.76 3.97]]\n",
            "\n",
            "  [[2.51 1.27 2.94 3.02]\n",
            "   [3.1  1.59 3.08 3.27]\n",
            "   [2.11 1.84 2.63 2.75]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up this way, we can now calculate the outputs from all attention heads with a single call to our self-attention operation."
      ],
      "metadata": {
        "id": "znSrQtwjD5BS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_heads_output, all_attn_weights = scaled_dot_product_attention(q_s_transposed,\n",
        "                                                                  k_s_transposed,\n",
        "                                                                  v_s_transposed)\n",
        "print(\"Self attention output:\\n\", all_heads_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeY2eY8oDqnI",
        "outputId": "dcb1928d-634e-457c-c790-578c2af0fb53"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Self attention output:\n",
            " tf.Tensor(\n",
            "[[[[2.833825  3.8457968 3.3957014 3.7308974]\n",
            "   [2.8301964 3.8441498 3.4033847 3.7260363]\n",
            "   [2.8210227 3.8409605 3.422514  3.7145672]]\n",
            "\n",
            "  [[2.4284096 2.3275397 3.0384262 4.0102625]\n",
            "   [2.441395  2.312757  3.039845  4.000343 ]\n",
            "   [2.4330301 2.3443832 3.0244172 4.004699 ]]\n",
            "\n",
            "  [[3.0552158 1.5740515 3.0670934 3.2499583]\n",
            "   [3.0592449 1.5751076 3.068358  3.2518098]\n",
            "   [3.036745  1.5701491 3.061039  3.2413504]]]], shape=(1, 3, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As a sanity check, we can compare this against the outputs from individual heads we calculated earlier:\n",
        "print(\"Per head outputs from using separate sets of weights per head:\")\n",
        "print(out0, \"\\n\")\n",
        "print(out1, \"\\n\")\n",
        "print(out2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnqMpQwGEGyI",
        "outputId": "8777b7f4-90f7-4e85-c3bd-2a464ad65072"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Per head outputs from using separate sets of weights per head:\n",
            "tf.Tensor(\n",
            "[[[2.833825  3.8457968 3.3957014 3.7308974]\n",
            "  [2.8301964 3.8441498 3.4033847 3.7260363]\n",
            "  [2.8210227 3.8409605 3.422514  3.7145672]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "tf.Tensor(\n",
            "[[[2.4284098 2.32754   3.0384266 4.010263 ]\n",
            "  [2.441395  2.312757  3.039845  4.000343 ]\n",
            "  [2.4330301 2.3443832 3.0244172 4.004699 ]]], shape=(1, 3, 4), dtype=float32) \n",
            "\n",
            "tf.Tensor(\n",
            "[[[3.0552158 1.5740515 3.0670934 3.2499583]\n",
            "  [3.0592449 1.5751076 3.068358  3.2518098]\n",
            "  [3.036745  1.5701491 3.061039  3.2413504]]], shape=(1, 3, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the final concatenated result, we need to reverse our **reshape** and **transpose** operation, starting with the **transpose** this time."
      ],
      "metadata": {
        "id": "wTHxdOTJETQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_out_b = tf.reshape(tf.transpose(all_heads_output, perm=[0, 2, 1, 3]),\n",
        "                            shape=(batch_size, seq_len, embed_dim))\n",
        "\n",
        "print(\"Final output from using single query, key, value matrices:\\n\", \n",
        "      combined_out_b, \"\\n\")\n",
        "print(\"Final output from using separate query, key, value matrices per head:\\n\", \n",
        "      combined_out_a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vzZQPAJEO8H",
        "outputId": "90a09779-f495-4cc7-cd2b-3ca025e954d5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output from using single query, key, value matrices:\n",
            " tf.Tensor(\n",
            "[[[2.833825  3.8457968 3.3957014 3.7308974 2.4284096 2.3275397 3.0384262\n",
            "   4.0102625 3.0552158 1.5740515 3.0670934 3.2499583]\n",
            "  [2.8301964 3.8441498 3.4033847 3.7260363 2.441395  2.312757  3.039845\n",
            "   4.000343  3.0592449 1.5751076 3.068358  3.2518098]\n",
            "  [2.8210227 3.8409605 3.422514  3.7145672 2.4330301 2.3443832 3.0244172\n",
            "   4.004699  3.036745  1.5701491 3.061039  3.2413504]]], shape=(1, 3, 12), dtype=float32) \n",
            "\n",
            "Final output from using separate query, key, value matrices per head:\n",
            " [[[2.833825  3.8457968 3.3957014 3.7308974 2.4284098 2.32754   3.0384266\n",
            "   4.010263  3.0552158 1.5740515 3.0670934 3.2499583]\n",
            "  [2.8301964 3.8441498 3.4033847 3.7260363 2.441395  2.312757  3.039845\n",
            "   4.000343  3.0592449 1.5751076 3.068358  3.2518098]\n",
            "  [2.8210227 3.8409605 3.422514  3.7145672 2.4330301 2.3443832 3.0244172\n",
            "   4.004699  3.036745  1.5701491 3.061039  3.2413504]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting everything together\n",
        "We can encapsulate everything we just covered in a class."
      ],
      "metadata": {
        "id": "d17GO1sTE4Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model  # model dimension\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.d_head = self.d_model // self.num_heads # head_dim\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(self.d_model)\n",
        "    self.wk = tf.keras.layers.Dense(self.d_model)\n",
        "    self.wv = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "    # Linear layer to generate the final output \n",
        "    self.dense = tf.keras.layers.Dense(self.d_model)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    split_inputs = tf.reshape(x, (batch_size, -1, self.num_heads, self.d_head))\n",
        "    return tf.transpose(split_inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def merge_heads(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    merged_inputs = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    return tf.reshape(merged_inputs, (batch_size, -1, self.d_model))\n",
        "  \n",
        "  def call(self, q, k, v, mask):\n",
        "    qs = self.wq(q)\n",
        "    ks =self.wk(k)\n",
        "    vs = self.wv(v)\n",
        "    \n",
        "    qs = self.split_heads(qs)\n",
        "    ks = self.split_heads(ks)\n",
        "    vs = self.split_heads(vs)\n",
        "\n",
        "    output, attn_weights = scaled_dot_product_attention(qs, ks, vs, mask)\n",
        "    output = self.merge_heads(output)\n",
        "\n",
        "    return self.dense(output), attn_weights"
      ],
      "metadata": {
        "id": "v9NLyAfpEk_I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check\n",
        "mhsa = MultiHeadSelfAttention(12, 3)\n",
        "\n",
        "output, attn_weights = mhsa(x, x, x, None)\n",
        "print(f\"MHSA output{output.shape}:\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5WAyyhOFUVz",
        "outputId": "962bdb2c-f961-43e1-f86b-886a406ed6c0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MHSA output(1, 3, 12):\n",
            "tf.Tensor(\n",
            "[[[-1.0467246  -0.38746828 -0.00595006 -0.3837807  -0.24663472\n",
            "   -0.2171552   0.18601058  0.33198413 -0.30640054  0.04098597\n",
            "    0.09575015 -0.3390737 ]\n",
            "  [-1.0250382  -0.38153133 -0.01544506 -0.34862208 -0.2214207\n",
            "   -0.19764036  0.17804733  0.38613892 -0.30171084  0.00365527\n",
            "    0.07438526 -0.2792571 ]\n",
            "  [-1.0018137  -0.3578776   0.03382985 -0.4144197  -0.28377312\n",
            "   -0.24793994  0.20015937  0.36169147 -0.32287133  0.05772191\n",
            "    0.15491143 -0.31276175]]], shape=(1, 3, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Block\n",
        "\n",
        "\n",
        "We can now build our **Encoder Block**. In addition to the **Multi-Head Self Attention** layer, the **Encoder Block** also has **skip connections**, **layer normalization steps**, and a **two-layer feed-forward neural network**. The original **Attention Is All You Need** paper also included some **dropout** applied to the self-attention output which isn't shown in the illustration below (see references for a link to the paper).\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1D8sLDyQMqqhCjHWOn-I7rZKHugWxFyLy\" width=\"500\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "KIjkreu2Hq3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since a two-layer feed forward neural network is used in multiple places in the transformer, here's a function which creates and returns one."
      ],
      "metadata": {
        "id": "E6f1OX6laMUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.models.Sequential == tf.keras.Sequential"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpBQdLeyazjI",
        "outputId": "a5fd35a0-72c9-4f16-891a-52ccfcd424ad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def feed_forward_network(d_model, hidden_dim):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(hidden_dim, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ],
      "metadata": {
        "id": "yVntpTQxHc-H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our encoder block containing all the layers and steps from the preceding illustration (plus dropout)."
      ],
      "metadata": {
        "id": "7xGv8fMxcj02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.mhsa = MultiHeadSelfAttention(d_model, num_heads)\n",
        "    self.ffn  = feed_forward_network(d_model, hidden_dim)\n",
        "\n",
        "    self.dropout   = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    mhsa_output, attn_weights = self.mhsa(x, x, x, mask)\n",
        "    mhsa_output = self.dropout(mhsa_output, training=training)\n",
        "    mhsa_output = self.layernorm(x + mhsa_output)\n",
        "    \n",
        "    ffn_output = self.ffn(mhsa_output)\n",
        "    ffn_output = self.dropout(ffn_output, training=training)\n",
        "    output     = self.layernorm(ffn_output + mhsa_output)\n",
        "    \n",
        "    return output, attn_weights\n"
      ],
      "metadata": {
        "id": "gnhPX0FuckEO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have an embedding dimension of 12, and we want 3 attention heads and a feed forward network with a hidden dimension of 48 (4x the embedding dimension). We would declare and use a single encoder block like so:"
      ],
      "metadata": {
        "id": "MulXV_14eqZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_block = EncoderBlock(12, 3, 48)\n",
        "\n",
        "block_output,  _ = encoder_block(x, True, None)\n",
        "print(f\"Output from single encoder block {block_output.shape}:\")\n",
        "print(block_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xptFCdwxaL-k",
        "outputId": "15a33925-21de-432d-94e1-55e7059dac18"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from single encoder block (1, 3, 12):\n",
            "tf.Tensor(\n",
            "[[[-0.3716895   0.9727424   0.59347695  0.9779479  -0.83920187\n",
            "    0.04553704  1.6231296  -0.6519545  -0.5456049   0.65776443\n",
            "   -0.21320732 -2.2489407 ]\n",
            "  [-0.19393569  0.69055057  0.01404015  0.87591046 -0.6793242\n",
            "    0.39865723  1.7146664  -0.03991906 -1.5244924   1.192019\n",
            "   -0.7731841  -1.6749882 ]\n",
            "  [-0.1703595   1.1842399  -0.554807    0.04148576 -0.15297136\n",
            "   -0.34296605 -0.45370454 -0.7941657  -0.20880504  2.1104808\n",
            "    1.1808958  -1.8393229 ]]], shape=(1, 3, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word and Positional Embeddings\n",
        "\n",
        "Let's now deal with the actual input to the **initial** encoder block. The inputs are going to be *positional word embeddings*. That is, word embeddings with some positional information added to them.\n",
        "<br>\n",
        "\n",
        "Let's start with **subword** tokenization. For demonstration, we'll use a subword tokenizer called **BPEmb**. It uses **Byte-Pair Encoding** and supports over two hundred languages. \n",
        "\n",
        "https://bpemb.h-its.org/\n"
      ],
      "metadata": {
        "id": "SE9TYeXEpcAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the English tokenizer.\n",
        "bpemb_en = BPEmb(lang='en')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIq_hKGWaLxk",
        "outputId": "86f2ef33-2250-4fba-fb6e-b610a59b06dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400869/400869 [00:00<00:00, 657807.23B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3784656/3784656 [00:01<00:00, 3455090.31B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The library comes with embeddings for a number of words\n",
        "bpemb_vocab_size, bpemb_embed_size = bpemb_en.vectors.shape\n",
        "\n",
        "print(\"Vocabulary size:\", bpemb_vocab_size)\n",
        "print(\"Embedding size:\", bpemb_embed_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2IMkhA3aLuZ",
        "outputId": "64d72a00-2df2-4e05-fa69-d60026b5a2e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 10000\n",
            "Embedding size: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding for the word \"car\"\n",
        "bpemb_en.vectors[bpemb_en.words.index('car')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPH1wgQqaLrn",
        "outputId": "b6eb43c5-0357-4e2a-af5e-71cda53958c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.305548, -0.325598, -0.134716, -0.078735, -0.660545,  0.076211,\n",
              "       -0.735487,  0.124533, -0.294402,  0.459688,  0.030137,  0.174041,\n",
              "       -0.224223,  0.486189, -0.504649, -0.459699,  0.315747,  0.477885,\n",
              "        0.091398,  0.427867,  0.016524, -0.076833, -0.899727,  0.493158,\n",
              "       -0.022309, -0.422785, -0.154148,  0.204981,  0.379834,  0.070588,\n",
              "        0.196073, -0.368222,  0.473406,  0.007409,  0.004303, -0.007823,\n",
              "       -0.19103 , -0.202509,  0.109878, -0.224521, -0.35741 , -0.611633,\n",
              "        0.329958, -0.212956, -0.497499, -0.393839, -0.130101, -0.216903,\n",
              "       -0.105595, -0.076007, -0.483942, -0.139704, -0.161647,  0.136985,\n",
              "        0.415363, -0.360143,  0.038601, -0.078804, -0.030421,  0.324129,\n",
              "        0.223378, -0.523636, -0.048317, -0.032248, -0.117367,  0.470519,\n",
              "        0.225816, -0.222065, -0.225007, -0.165904, -0.334389, -0.20157 ,\n",
              "        0.572352, -0.268794,  0.301929, -0.005563,  0.387491,  0.261031,\n",
              "       -0.11613 ,  0.074982, -0.008433,  0.259987, -0.099893, -0.268875,\n",
              "       -0.054047, -0.534776, -0.111101, -0.051742,  0.214114,  0.04293 ,\n",
              "        0.039873, -0.453112,  0.087382, -0.333201, -0.034079, -0.833045,\n",
              "        0.155232, -1.132393, -0.294766,  0.327572], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't need the embeddings since we're going to use our own embedding layer. What we're interested in are the subword tokens and their respective ids. The ids will be used as indexes into our embedding layer.<br>\n",
        "\n",
        " **BPEmb** places underscores in front of any tokens which are whole words or intended to begin words.<br>\n",
        "\n",
        "Remember that subword tokenizers are trained using count frequencies over a corpus. So these subword tokens are specific to **BPEmb**. Another subword tokenizer may output something different. This is why it's important that when we use a pretrained model, we make sure to use the pretrained model's tokenizer. "
      ],
      "metadata": {
        "id": "ZZLCu8bUq3b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"Where can I find a pizzeria?\"\n",
        "tokens = bpemb_en.encode(sample_sentence)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCJG8PghrJwZ",
        "outputId": "c83c49cc-34f3-44c6-b894-391eb53bc9c1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁where', '▁can', '▁i', '▁find', '▁a', '▁p', 'iz', 'zer', 'ia', '?']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can retrieve each subword token's respective id using the *encode_ids* method\n",
        "token_seq = np.array(bpemb_en.encode_ids('Where can I find a pizzeria?'))\n",
        "token_seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMW8TstZrJtl",
        "outputId": "88f5c106-2207-4219-b35b-7b5d686e0a03"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 571,  280,  386, 1934,    4,   24,  248, 4339,  177, 9967])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a way to tokenize and vectorize sentences, we can declare and use an embedding layer with the same vocabulary size as **BPEmb** and a desired embedding size."
      ],
      "metadata": {
        "id": "PQq9VBfVr4im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embed = tf.keras.layers.Embedding(bpemb_vocab_size, embed_dim)\n",
        "token_embeddings = token_embed(token_seq)\n",
        "\n",
        "# The untrained embeddings for our sample sentence.\n",
        "print(\"Embeddings for: \", sample_sentence)\n",
        "print(token_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNfRMjhFrJrJ",
        "outputId": "388047e6-07cd-40d5-d683-a4ff8a9592d1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings for:  Where can I find a pizzeria?\n",
            "tf.Tensor(\n",
            "[[ 0.01448974 -0.03366766 -0.00832275  0.01385177  0.04119352 -0.00592104\n",
            "  -0.04922706 -0.0168265   0.03531576  0.02216787 -0.04024782 -0.04667196]\n",
            " [-0.04727457  0.01700499  0.0381448  -0.03491439  0.02874904 -0.00526644\n",
            "   0.02942361  0.00758667 -0.00871487 -0.02891201  0.02335632 -0.02620664]\n",
            " [ 0.04148301 -0.03412551 -0.00809424 -0.00841127 -0.03646825  0.03917141\n",
            "  -0.03420101 -0.02235417  0.03981816 -0.01802616 -0.00026751 -0.00799946]\n",
            " [ 0.04622055 -0.04998402  0.020508    0.02776426 -0.00971972  0.02428455\n",
            "  -0.01700937  0.00117232  0.03432995  0.03076286  0.02077112 -0.03850199]\n",
            " [-0.02762603 -0.00742032 -0.02562984  0.02848845 -0.0149024  -0.0446\n",
            "  -0.03575293  0.04781846  0.01220576 -0.01164117  0.04130632 -0.04780921]\n",
            " [ 0.04084361  0.01199125 -0.04959185 -0.0034986   0.01827468  0.00143937\n",
            "   0.00865041  0.04814276 -0.03797473  0.04411812 -0.01110473  0.03618764]\n",
            " [-0.04814401 -0.01833753 -0.00677134  0.03797131  0.03364751  0.04308169\n",
            "   0.02716609  0.0305582  -0.02443091 -0.00351803  0.04415857 -0.00049019]\n",
            " [ 0.01459445 -0.0304832   0.01104163 -0.01177107  0.02881538 -0.02467188\n",
            "   0.02818072  0.00184544  0.03394151  0.04626587 -0.0064707  -0.04826766]\n",
            " [ 0.03276045 -0.04748629  0.01445748 -0.01749393 -0.04358682 -0.02466159\n",
            "  -0.00990021  0.01761565 -0.03135338 -0.02656146  0.02987807 -0.03315022]\n",
            " [-0.02679788 -0.01144725 -0.04189699  0.03215503  0.04524245 -0.0029672\n",
            "   0.03325656 -0.0484347   0.01218249 -0.03196283 -0.00090467  0.02919737]], shape=(10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to add *positional* information to each token embedding. The original paper used sinusoidals but it's more common these days to just use another set of embeddings. We'll do the latter here.<br>\n",
        "\n",
        "Here, we're declaring an embedding layer with rows equalling a maximum sequence length and columns equalling our token embedding size. We then generate a vector of position ids."
      ],
      "metadata": {
        "id": "svx-tEXtsh-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_len = 256\n",
        "pos_embed = tf.keras.layers.Embedding(max_seq_len, embed_dim)\n",
        "\n",
        "# Generate ids for each position of the token sequence.\n",
        "pos_idx = tf.range(len(token_seq))\n",
        "pos_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "Gn7CpESWaLjv",
        "outputId": "dc64111e-6d75-43d6-fd5d-a327bf66d098"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-297de572db28>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Generate ids for each position of the token sequence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpos_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpos_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'token_seq' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use these position ids to index into the positional embedding layer."
      ],
      "metadata": {
        "id": "mOeeMJtItQ5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are our positon embeddings\n",
        "position_embeddings = pos_embed(pos_idx)\n",
        "print(\"Position embeddings for the input sequence\\n\", position_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enzQeRZ6aLhZ",
        "outputId": "c107f516-6808-46d1-d2dc-d1326610bd8c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Position embeddings for the input sequence\n",
            " tf.Tensor(\n",
            "[[-1.13660097e-03 -1.60347335e-02  9.01959091e-03 -3.10138948e-02\n",
            "   1.29749440e-02 -7.72383064e-03  1.52164139e-02 -3.39352638e-02\n",
            "   7.02438504e-03  6.73501566e-03  2.41510570e-05 -1.55613795e-02]\n",
            " [-3.57269123e-03 -2.86044478e-02 -3.23886275e-02 -1.80147588e-04\n",
            "  -3.89828458e-02 -1.98534876e-03  4.17050458e-02  2.29485370e-02\n",
            "   1.35246851e-02 -3.60701457e-02 -6.39129430e-04  1.47822164e-02]\n",
            " [-7.78924301e-03  4.42704298e-02  1.64815225e-02  1.85513832e-02\n",
            "  -2.62727737e-02  1.85221471e-02  1.23622529e-02  9.27203894e-03\n",
            "  -2.87427437e-02 -4.18234617e-04  5.71832061e-05  2.09636204e-02]\n",
            " [-1.27251260e-02 -4.71079014e-02 -3.82656343e-02 -3.47413048e-02\n",
            "   3.87997739e-02  2.04805396e-02  3.33256014e-02  1.59940384e-02\n",
            "   2.13720463e-02  4.19230796e-02  3.15302126e-02  2.68095471e-02]\n",
            " [ 3.52654196e-02 -2.17069387e-02  4.10317667e-02  1.63805820e-02\n",
            "  -1.80840120e-02 -2.24395879e-02 -2.03658100e-02 -4.42672260e-02\n",
            "  -9.07685608e-03 -2.56793145e-02 -2.78522130e-02  6.86514378e-03]\n",
            " [-2.08618883e-02 -3.70792300e-03  3.44213136e-02 -4.79813218e-02\n",
            "  -2.28895191e-02  4.81403358e-02 -4.35671099e-02 -4.92952764e-04\n",
            "   2.39275582e-02  1.86514743e-02  4.10373919e-02 -1.90468188e-02]\n",
            " [-4.65556644e-02  3.77149470e-02  1.54256858e-02 -2.49438360e-03\n",
            "  -4.71875817e-03 -2.60186084e-02 -9.08249617e-03 -4.18593064e-02\n",
            "   1.26916207e-02  2.98520662e-02 -2.65158415e-02 -4.27721031e-02]\n",
            " [-4.36103009e-02  3.77787389e-02 -4.24773097e-02  1.39093399e-03\n",
            "   3.38524245e-02 -1.49531253e-02 -3.42829227e-02  1.61580481e-02\n",
            "  -1.80349126e-02  9.95836407e-03  4.10156585e-02 -4.92387898e-02]\n",
            " [ 3.13102119e-02 -1.18730888e-02  6.81263208e-03 -1.25336647e-03\n",
            "  -3.64179984e-02 -4.96743694e-02 -2.55889427e-02 -1.40004531e-02\n",
            "   4.52695824e-02  3.41542698e-02  4.16408069e-02  4.50129174e-02]\n",
            " [-4.50006127e-02  4.74971272e-02 -3.52361314e-02 -4.74363342e-02\n",
            "   2.58148946e-02 -4.02015932e-02  4.03168909e-02  1.81047358e-02\n",
            "   4.96347584e-02 -1.60942301e-02 -1.27878077e-02 -4.22639959e-02]], shape=(10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final step is to add our token and position embeddings. The result will be the input to the first encoder block."
      ],
      "metadata": {
        "id": "ltEvRNBztmnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = token_embeddings + position_embeddings\n",
        "print(\"Input to the initial encoder block:\\n\", input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP38bEJ0r602",
        "outputId": "e9216f7b-95f8-4f58-ddf0-705cb6ef6970"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input to the initial encoder block:\n",
            " tf.Tensor(\n",
            "[[ 0.01335314 -0.04970239  0.00069684 -0.01716213  0.05416846 -0.01364487\n",
            "  -0.03401065 -0.05076176  0.04234014  0.02890289 -0.04022367 -0.06223334]\n",
            " [-0.05084726 -0.01159946  0.00575617 -0.03509453 -0.01023381 -0.00725179\n",
            "   0.07112865  0.03053521  0.00480982 -0.06498215  0.02271719 -0.01142442]\n",
            " [ 0.03369377  0.01014492  0.00838728  0.01014012 -0.06274102  0.05769356\n",
            "  -0.02183876 -0.01308214  0.01107541 -0.0184444  -0.00021032  0.01296416]\n",
            " [ 0.03349543 -0.09709191 -0.01775763 -0.00697704  0.02908006  0.04476508\n",
            "   0.01631624  0.01716635  0.055702    0.07268594  0.05230133 -0.01169244]\n",
            " [ 0.00763939 -0.02912726  0.01540192  0.04486903 -0.03298641 -0.06703959\n",
            "  -0.05611874  0.00355123  0.0031289  -0.03732048  0.01345411 -0.04094407]\n",
            " [ 0.01998172  0.00828333 -0.01517054 -0.05147992 -0.00461484  0.04957971\n",
            "  -0.0349167   0.04764981 -0.01404717  0.06276959  0.02993267  0.01714082]\n",
            " [-0.09469967  0.01937742  0.00865435  0.03547692  0.02892875  0.01706308\n",
            "   0.0180836  -0.0113011  -0.01173929  0.02633403  0.01764273 -0.04326229]\n",
            " [-0.02901585  0.00729554 -0.03143568 -0.01038014  0.0626678  -0.039625\n",
            "  -0.0061022   0.01800349  0.0159066   0.05622423  0.03454496 -0.09750645]\n",
            " [ 0.06407066 -0.05935938  0.02127011 -0.01874729 -0.08000481 -0.07433596\n",
            "  -0.03548916  0.0036152   0.01391621  0.00759281  0.07151888  0.0118627 ]\n",
            " [-0.07179849  0.03604988 -0.07713312 -0.01528131  0.07105735 -0.04316879\n",
            "   0.07357345 -0.03032997  0.06181724 -0.04805706 -0.01369248 -0.01306663]], shape=(10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder\n",
        "\n",
        "Now that we have an encoder block and a way to embed our tokens with position information, we can create the **encoder** itself.<br>\n",
        "\n",
        "Given a batch of vectorized sequences, the encoder creates positional embeddings, runs them through its encoder blocks, and returns contextualized tokens."
      ],
      "metadata": {
        "id": "vTCkWEDsujjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, src_vocab_size, max_seq_len, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    self.token_embed = tf.keras.layers.Embedding(src_vocab_size, self.d_model)\n",
        "    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, self.d_model)\n",
        "\n",
        "    # The original Attention Is All You Need paper applied dropout to the\n",
        "    # input before feeding it to the first encoder block\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    # Create encoder blocks\n",
        "    self.blocks = [EncoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate)\n",
        "                   for _ in range(num_blocks)]\n",
        "  \n",
        "  def call(self, input, training, mask):\n",
        "    token_embeds = self.token_embed(input)\n",
        "\n",
        "    # Generate position indices for a batch of input sequences\n",
        "    num_pos = input.shape[0] * self.max_seq_len\n",
        "    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
        "    pos_idx = np.reshape(pos_idx, input.shape)\n",
        "    pos_embeds = self.pos_embed(pos_idx)\n",
        "\n",
        "    x = self.dropout(token_embeds + pos_embeds, training=training)\n",
        "    \n",
        "    # Run input through successive encoder blocks\n",
        "    for block in self.blocks:\n",
        "      x, weights = block(x, training, mask)\n",
        "    \n",
        "    return x, weights\n"
      ],
      "metadata": {
        "id": "6raG1r8pr6tl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're wondering about this code block here:\n",
        "\n",
        "\n",
        "```\n",
        "num_pos = input.shape[0] * self.max_seq_len\n",
        "pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
        "pos_idx = np.reshape(pos_idx, input.shape)\n",
        "pos_embeds = self.pos_embed(pos_idx)\n",
        "```\n",
        "\n",
        "\n",
        "This generates positional embeddings for a *batch* of input sequences. Suppose this was our batch of input sequences to the encoder."
      ],
      "metadata": {
        "id": "ulpUOkS9yo76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch of 3 sequences, each of length 10 (10 is also the \n",
        "# maximum sequence length in this case).\n",
        "seqs = np.random.randint(0, 10000, size=(3, 10))\n",
        "print(seqs.shape)\n",
        "print(seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0insnGsOr6mY",
        "outputId": "18e266fe-2260-47d9-ae4b-7fe8bfe2a36d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 10)\n",
            "[[7565 5471 5184 7659 4843 5351 3232 7140 2735 3036]\n",
            " [ 637 5675 6435 8349 1770 7142 8985 2513 6068 8600]\n",
            " [5482 4930  994  745 5065 2991 1543  804 9863 5955]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to retrieve a positional embedding for every element in this batch.\n",
        "# The first step is to create the respective positional ids...\n",
        "pos_ids = np.resize(np.arange(seqs.shape[1]), seqs.shape[0] * seqs.shape[1])\n",
        "print(pos_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgSvrWbvr6d4",
        "outputId": "7bd3287c-6bc4-47f9-c1c3-908c85bda6a2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ...and then reshape them to match the input batch dimensions\n",
        "pos_ids = np.reshape(pos_ids, (3, 10))\n",
        "print(pos_ids.shape)\n",
        "print(pos_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EC2Mn_Xr6SM",
        "outputId": "d9888379-1d61-4104-e9ac-b7e0d55aaab8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 10)\n",
            "[[0 1 2 3 4 5 6 7 8 9]\n",
            " [0 1 2 3 4 5 6 7 8 9]\n",
            " [0 1 2 3 4 5 6 7 8 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now retrieve position embeddings for every token embedding\n",
        "pos_embed(pos_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYcZla3Eswaa",
        "outputId": "148a8ad3-9bae-44f9-ace9-068d810f6bb4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10, 12), dtype=float32, numpy=\n",
              "array([[[-0.04305434, -0.02453849, -0.02423087, -0.00924901,\n",
              "         -0.0244591 , -0.02212497,  0.02808353, -0.02292987,\n",
              "          0.02081816,  0.01929775,  0.04014203, -0.03314801],\n",
              "        [ 0.02030656, -0.0497223 , -0.00140219,  0.01579145,\n",
              "          0.02903452, -0.02081794, -0.00895287,  0.02823159,\n",
              "         -0.01854497, -0.00218279,  0.02976627, -0.03864694],\n",
              "        [-0.01225422, -0.04528918,  0.04851009,  0.01066749,\n",
              "          0.01510625, -0.00701993, -0.00150501, -0.00867059,\n",
              "          0.0139587 , -0.02797397, -0.04383787, -0.00570369],\n",
              "        [-0.03308626,  0.00466092, -0.02672722,  0.03316107,\n",
              "          0.0490361 , -0.02492866,  0.0132092 , -0.00282953,\n",
              "          0.04492934,  0.02720095, -0.03943354,  0.00880854],\n",
              "        [ 0.0128976 , -0.0209771 ,  0.04239047, -0.04588555,\n",
              "          0.03267778,  0.04865321,  0.00468792, -0.01059624,\n",
              "         -0.01658805,  0.02663327, -0.01916671,  0.03830277],\n",
              "        [-0.02320021, -0.0400533 , -0.03976786,  0.01714838,\n",
              "         -0.01293249, -0.00791768, -0.02483496,  0.03666281,\n",
              "         -0.03127867, -0.04416982, -0.00784333,  0.01464247],\n",
              "        [-0.0046326 ,  0.01450325,  0.01118956,  0.0433962 ,\n",
              "          0.0454076 , -0.00773064,  0.02121493, -0.02530961,\n",
              "         -0.0007352 ,  0.02098644,  0.04417558,  0.00256656],\n",
              "        [ 0.03459842, -0.01927989, -0.02191262,  0.01840377,\n",
              "          0.00254694, -0.03689688, -0.02231888, -0.02320508,\n",
              "          0.04532664, -0.04465071,  0.02768251, -0.04491109],\n",
              "        [-0.02640728,  0.01146618,  0.02385351, -0.03332501,\n",
              "         -0.00498819,  0.0149412 ,  0.03271415,  0.01476754,\n",
              "         -0.02059666,  0.02227039,  0.00290896,  0.0166947 ],\n",
              "        [ 0.04180323, -0.04023404, -0.01505473, -0.01969638,\n",
              "         -0.041557  , -0.04112086, -0.00851616, -0.03608746,\n",
              "         -0.04526066, -0.01537525,  0.0257921 ,  0.03474596]],\n",
              "\n",
              "       [[-0.04305434, -0.02453849, -0.02423087, -0.00924901,\n",
              "         -0.0244591 , -0.02212497,  0.02808353, -0.02292987,\n",
              "          0.02081816,  0.01929775,  0.04014203, -0.03314801],\n",
              "        [ 0.02030656, -0.0497223 , -0.00140219,  0.01579145,\n",
              "          0.02903452, -0.02081794, -0.00895287,  0.02823159,\n",
              "         -0.01854497, -0.00218279,  0.02976627, -0.03864694],\n",
              "        [-0.01225422, -0.04528918,  0.04851009,  0.01066749,\n",
              "          0.01510625, -0.00701993, -0.00150501, -0.00867059,\n",
              "          0.0139587 , -0.02797397, -0.04383787, -0.00570369],\n",
              "        [-0.03308626,  0.00466092, -0.02672722,  0.03316107,\n",
              "          0.0490361 , -0.02492866,  0.0132092 , -0.00282953,\n",
              "          0.04492934,  0.02720095, -0.03943354,  0.00880854],\n",
              "        [ 0.0128976 , -0.0209771 ,  0.04239047, -0.04588555,\n",
              "          0.03267778,  0.04865321,  0.00468792, -0.01059624,\n",
              "         -0.01658805,  0.02663327, -0.01916671,  0.03830277],\n",
              "        [-0.02320021, -0.0400533 , -0.03976786,  0.01714838,\n",
              "         -0.01293249, -0.00791768, -0.02483496,  0.03666281,\n",
              "         -0.03127867, -0.04416982, -0.00784333,  0.01464247],\n",
              "        [-0.0046326 ,  0.01450325,  0.01118956,  0.0433962 ,\n",
              "          0.0454076 , -0.00773064,  0.02121493, -0.02530961,\n",
              "         -0.0007352 ,  0.02098644,  0.04417558,  0.00256656],\n",
              "        [ 0.03459842, -0.01927989, -0.02191262,  0.01840377,\n",
              "          0.00254694, -0.03689688, -0.02231888, -0.02320508,\n",
              "          0.04532664, -0.04465071,  0.02768251, -0.04491109],\n",
              "        [-0.02640728,  0.01146618,  0.02385351, -0.03332501,\n",
              "         -0.00498819,  0.0149412 ,  0.03271415,  0.01476754,\n",
              "         -0.02059666,  0.02227039,  0.00290896,  0.0166947 ],\n",
              "        [ 0.04180323, -0.04023404, -0.01505473, -0.01969638,\n",
              "         -0.041557  , -0.04112086, -0.00851616, -0.03608746,\n",
              "         -0.04526066, -0.01537525,  0.0257921 ,  0.03474596]],\n",
              "\n",
              "       [[-0.04305434, -0.02453849, -0.02423087, -0.00924901,\n",
              "         -0.0244591 , -0.02212497,  0.02808353, -0.02292987,\n",
              "          0.02081816,  0.01929775,  0.04014203, -0.03314801],\n",
              "        [ 0.02030656, -0.0497223 , -0.00140219,  0.01579145,\n",
              "          0.02903452, -0.02081794, -0.00895287,  0.02823159,\n",
              "         -0.01854497, -0.00218279,  0.02976627, -0.03864694],\n",
              "        [-0.01225422, -0.04528918,  0.04851009,  0.01066749,\n",
              "          0.01510625, -0.00701993, -0.00150501, -0.00867059,\n",
              "          0.0139587 , -0.02797397, -0.04383787, -0.00570369],\n",
              "        [-0.03308626,  0.00466092, -0.02672722,  0.03316107,\n",
              "          0.0490361 , -0.02492866,  0.0132092 , -0.00282953,\n",
              "          0.04492934,  0.02720095, -0.03943354,  0.00880854],\n",
              "        [ 0.0128976 , -0.0209771 ,  0.04239047, -0.04588555,\n",
              "          0.03267778,  0.04865321,  0.00468792, -0.01059624,\n",
              "         -0.01658805,  0.02663327, -0.01916671,  0.03830277],\n",
              "        [-0.02320021, -0.0400533 , -0.03976786,  0.01714838,\n",
              "         -0.01293249, -0.00791768, -0.02483496,  0.03666281,\n",
              "         -0.03127867, -0.04416982, -0.00784333,  0.01464247],\n",
              "        [-0.0046326 ,  0.01450325,  0.01118956,  0.0433962 ,\n",
              "          0.0454076 , -0.00773064,  0.02121493, -0.02530961,\n",
              "         -0.0007352 ,  0.02098644,  0.04417558,  0.00256656],\n",
              "        [ 0.03459842, -0.01927989, -0.02191262,  0.01840377,\n",
              "          0.00254694, -0.03689688, -0.02231888, -0.02320508,\n",
              "          0.04532664, -0.04465071,  0.02768251, -0.04491109],\n",
              "        [-0.02640728,  0.01146618,  0.02385351, -0.03332501,\n",
              "         -0.00498819,  0.0149412 ,  0.03271415,  0.01476754,\n",
              "         -0.02059666,  0.02227039,  0.00290896,  0.0166947 ],\n",
              "        [ 0.04180323, -0.04023404, -0.01505473, -0.01969638,\n",
              "         -0.041557  , -0.04112086, -0.00851616, -0.03608746,\n",
              "         -0.04526066, -0.01537525,  0.0257921 ,  0.03474596]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try our encoder on a batch of sentences\n",
        "input_batch = [\n",
        "    \"Where can I find a pizzeria?\",\n",
        "    \"Mass hysteria over listeria.\",\n",
        "    \"I ain't no circle back girl.\"\n",
        "]\n",
        "\n",
        "bpemb_en.encode(input_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbLNSdc2swXI",
        "outputId": "a89db83c-3979-42e7-ffb3-3768cd2bfbdf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['▁where', '▁can', '▁i', '▁find', '▁a', '▁p', 'iz', 'zer', 'ia', '?'],\n",
              " ['▁mass', '▁hy', 'ster', 'ia', '▁over', '▁l', 'ister', 'ia', '.'],\n",
              " ['▁i', '▁a', 'in', \"'\", 't', '▁no', '▁circle', '▁back', '▁girl', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_seqs = bpemb_en.encode_ids(input_batch)\n",
        "print(\"Vectorized inputs:\")\n",
        "input_seqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdzEBrNkswQn",
        "outputId": "23639c25-ad43-4850-a2a9-6b1ad8ca437d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vectorized inputs:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[571, 280, 386, 1934, 4, 24, 248, 4339, 177, 9967],\n",
              " [1535, 1354, 1238, 177, 380, 43, 871, 177, 9935],\n",
              " [386, 4, 6, 9937, 9915, 467, 5410, 810, 3692, 9935]]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note how the input sequences aren't the same length in this batch.\n",
        "# In this case, we need to pad them out so that they are.\n",
        "padded_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(input_seqs, \n",
        "                                                                  padding='post',\n",
        "                                                                  truncating='post')\n",
        "print(\"Input to the encoder:\")\n",
        "print(padded_input_seqs.shape)\n",
        "print(padded_input_seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HZTDI8SswIB",
        "outputId": "a94239b3-67f9-4929-91c3-0b996d93d97f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input to the encoder:\n",
            "(3, 10)\n",
            "[[ 571  280  386 1934    4   24  248 4339  177 9967]\n",
            " [1535 1354 1238  177  380   43  871  177 9935    0]\n",
            " [ 386    4    6 9937 9915  467 5410  810 3692 9935]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our input now has padding, now's a good time to cover **masking**.\n",
        "<br>\n",
        "\n",
        "So given a mask, wherever there's a mask position set to 0, the corresponding position in the attention scores will be set to *-inf*. The resulting attention weight for the position will then be zero and no attending will occur for that position.\n",
        "<br>\n",
        "\n",
        "We covered *look-ahead* masks for the decoder to prevent it from attending to future tokens, but we also need masks for padding.\n",
        "<br>\n",
        "\n",
        "In total, there are three masks involved:\n",
        "1. The *encoder mask* to mask out any padding in the encoder sequences.\n",
        "\n",
        "2. The *decoder mask* which is used in the decoder's **first** multi-head self-attention layer. It's a <u>combination of two masks</u>: one to account for the padding in target sequences, and the look-ahead mask.\n",
        "\n",
        "3. The *memory mask* which is used in the decoder's **second** multi-head self-attention layer. The keys and values for this layer are going to be the encoder's output, and this mask will ensure the decoder doesn't attend to any encoder output which corresponds to padding. In practice, 1 and 3 are often the same.\n",
        "\n",
        "The *scaled_dot_product_attention* function has this line:\n",
        "```\n",
        "  if mask is not None:\n",
        "    scaled_scores = tf.where(mask==0, -np.inf, scaled_scores)\n",
        "```"
      ],
      "metadata": {
        "id": "BHMBsRDv1n18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create an encoder mask for our batch of input sequences.\n",
        "# Wherever there's padding, we want the mask position set to zero.\n",
        "enc_mask = tf.cast(tf.math.not_equal(padded_input_seqs, 0), tf.float32)\n",
        "print(\"Input:\")\n",
        "print(padded_input_seqs, '\\n')\n",
        "print(\"Encoder mask:\")\n",
        "print(enc_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWPFGiLj3QUC",
        "outputId": "a68d4982-7bb5-47c5-c576-be6936c75e57"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            "[[ 571  280  386 1934    4   24  248 4339  177 9967]\n",
            " [1535 1354 1238  177  380   43  871  177 9935    0]\n",
            " [ 386    4    6 9937 9915  467 5410  810 3692 9935]] \n",
            "\n",
            "Encoder mask:\n",
            "tf.Tensor(\n",
            "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]], shape=(3, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep in mind that the dimension of the attention matrix (for this example) is going to be:<br>\n",
        "*(batch size, number of heads, query size, key size)*<br>\n",
        "(3, 3, 10, 10)"
      ],
      "metadata": {
        "id": "SxrjKJZZ6QVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to expand the mask dimensions \n",
        "enc_mask = enc_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "enc_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpxe67ay3eBN",
        "outputId": "a2965f2a-a087-4aa4-bca0-a4481d3c10a2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 10), dtype=float32, numpy=\n",
              "array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This way, the encoder mask will now be *broadcasted*.<br>\n",
        "https://www.tensorflow.org/xla/broadcasting"
      ],
      "metadata": {
        "id": "5AgN1ysH6nVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can declare an encoder and pass it batches of vectorized sequences\n",
        "num_encoder_blocks = 6\n",
        "d_model = 12  # the embedding dimension used throughout\n",
        "num_heads = 3\n",
        "\n",
        "# Feed-forward network hidden dimension width\n",
        "ffn_hidden_dim = 48\n",
        "\n",
        "src_vocab_size = bpemb_vocab_size\n",
        "max_input_seq_len = padded_input_seqs.shape[1]\n",
        "\n",
        "encoder = Encoder(\n",
        "    num_encoder_blocks,\n",
        "    d_model,\n",
        "    num_heads,\n",
        "    ffn_hidden_dim,\n",
        "    src_vocab_size,\n",
        "    max_input_seq_len\n",
        ")"
      ],
      "metadata": {
        "id": "6wQG1jss0cTj"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now pass our input sequences and mask to the encoder\n",
        "encoder_output, attn_weights = encoder(padded_input_seqs, training=True, mask=enc_mask)\n",
        "\n",
        "print(f\"Encoder output {encoder_output.shape}:\")\n",
        "print(encoder_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZcGgD0V6nyy",
        "outputId": "c8db00f1-ff44-4886-c479-02ea09f42ec9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output (3, 10, 12):\n",
            "tf.Tensor(\n",
            "[[[ 0.92893213  1.1867573   0.19462499  0.39530596  0.00631281\n",
            "   -1.6360401  -0.7206076  -1.4299091   0.2929974  -1.35924\n",
            "    0.90086746  1.239999  ]\n",
            "  [-0.06947623  1.2353057  -0.2774222   0.81003     0.4535414\n",
            "   -0.608405   -0.47404522 -1.6943238   0.4667239  -1.4225894\n",
            "    1.9232383  -0.34257716]\n",
            "  [ 0.11077741  1.3144714   0.10531245  0.43197262  0.14090824\n",
            "   -1.4559658  -0.47059813 -1.5199034  -0.2603888  -1.1860625\n",
            "    1.2969326   1.4925439 ]\n",
            "  [ 0.9480911   1.3393903  -0.232166    0.6072952  -0.15742984\n",
            "   -1.2569174  -0.3015812  -1.4833101  -0.02259316 -1.5423868\n",
            "    1.4935411   0.6080667 ]\n",
            "  [ 0.8408628   0.90129286 -1.0122358   0.2840256  -0.70243484\n",
            "   -0.9836674  -0.251804   -0.8127806   0.63793546 -1.5873525\n",
            "    1.9116813   0.77447706]\n",
            "  [-0.01615997  2.3178575   0.23885302  0.55538577  0.17812982\n",
            "   -1.2513822  -0.99101365 -0.82830644 -0.88112456 -0.7975826\n",
            "    1.230519    0.24482422]\n",
            "  [ 1.5429044   0.6584737   0.2055447   0.5493315   0.5255403\n",
            "   -2.1298847  -0.6510709  -1.0220662   0.31756014 -1.1672629\n",
            "    1.0341659   0.13676402]\n",
            "  [-0.12864153  0.78882444  0.46143848  0.30084655  0.08729875\n",
            "   -1.4798224  -1.3893031  -1.4496895   0.6842392  -0.626843\n",
            "    1.0827876   1.6688652 ]\n",
            "  [-0.22681013  1.223798    0.26435268  0.33325788  0.4147283\n",
            "    0.44355938 -1.3992394  -2.4801233   0.18352771 -0.4101094\n",
            "    1.1531858   0.49987268]\n",
            "  [-0.286737    1.5112602   0.07253578  0.70539385  0.8729879\n",
            "   -0.6833717  -1.4172643  -1.5696044  -0.517347   -0.6484079\n",
            "    1.6172866   0.34326786]]\n",
            "\n",
            " [[-0.02034802  1.6143093   0.24245478 -0.14984854  0.2857834\n",
            "   -1.3579096  -0.8576703  -1.7088931   0.57782364 -0.83929634\n",
            "    1.4495189   0.7640761 ]\n",
            "  [-0.8540118   1.5181967   0.629134   -0.14419517  0.06201283\n",
            "   -1.2026441  -0.9269157  -1.3534801   0.20608106 -0.5257854\n",
            "    1.8963449   0.69526273]\n",
            "  [-0.2082983   2.0368173   0.21694575  0.04347005 -0.20483266\n",
            "   -1.4822264  -0.7520965  -1.5756547   0.07939396 -0.2644463\n",
            "    1.3398008   0.77112705]\n",
            "  [-0.34478828  1.9783678   0.07673113  0.20726295  0.44452184\n",
            "   -1.5355767  -1.1723925  -1.1235414   0.18607454 -0.2774562\n",
            "    1.6140563  -0.05325972]\n",
            "  [ 0.1836668   1.5335894   0.7001252  -0.5655535   0.28866965\n",
            "   -1.4713101  -0.6731765  -1.55045     0.06678305 -0.7853376\n",
            "    1.6279829   0.6450109 ]\n",
            "  [-0.86408657  2.1279793   0.80481386 -0.07189713 -0.15092325\n",
            "   -1.1827247  -0.9673833  -1.1237507  -0.12490632 -0.5018605\n",
            "    1.2060221   0.84871644]\n",
            "  [ 0.80172944  1.1860924   0.07447542 -0.09049428  0.06947599\n",
            "   -1.5652803  -0.5563301  -1.507592    0.5939946  -0.79892176\n",
            "    1.9671007  -0.17425035]\n",
            "  [-0.25171006  1.9177905   0.27027223 -0.01579664  0.7050126\n",
            "   -1.425047   -1.6146262  -1.057536    0.6974197  -0.5712719\n",
            "    0.99301445  0.35247833]\n",
            "  [ 0.6824731   1.8643999   0.06880321  0.28704292  0.50050855\n",
            "   -1.3255111  -0.9430738  -1.3114202   0.11830457 -0.8982618\n",
            "    1.4955218  -0.5387874 ]\n",
            "  [ 0.52169514  1.8352256  -0.23023735  0.05874982  0.3338441\n",
            "   -1.2397105  -1.6763728  -0.8999619   0.5444269  -0.4857245\n",
            "    1.5486125  -0.31054685]]\n",
            "\n",
            " [[-0.19792618  2.2103856   0.2292448  -0.40648174 -0.3165054\n",
            "   -1.72612    -0.682412   -0.84808385  0.58950603 -0.35021448\n",
            "    1.4531598   0.04544692]\n",
            "  [-0.51638424  2.485925   -0.0831072  -0.51322585  0.29855794\n",
            "   -0.46635786 -0.6240485  -1.3072535   0.53443766 -0.875855\n",
            "    1.3211361  -0.2538247 ]\n",
            "  [ 0.05271668  2.2454426   0.20883025 -0.36762744  0.11035027\n",
            "   -1.4616337  -1.1258279  -0.9633465   0.21716085 -0.29808772\n",
            "    1.5070826  -0.12505962]\n",
            "  [ 0.07342799  2.151788    0.3015096   0.0720748   0.28077236\n",
            "   -1.7170727  -0.72291076 -1.0349709  -0.05600403 -0.38226822\n",
            "    1.5061754  -0.4725214 ]\n",
            "  [-1.0739304   2.2634249   0.37815872 -0.6577444   0.16106719\n",
            "   -0.09427308 -1.1390586  -0.6501596   0.53863776 -0.8085623\n",
            "    1.5021973  -0.41975677]\n",
            "  [ 1.0766875   1.9784766  -0.29644668 -1.7988545  -0.5825025\n",
            "   -0.7808513  -0.89544505 -0.2608184   0.37135497  0.0280687\n",
            "    1.2721492  -0.11181851]\n",
            "  [ 0.33552364  2.110854    0.05114626 -0.3575446  -0.27192223\n",
            "   -0.39751184 -0.9098723  -1.3695874   0.22750552 -0.8066156\n",
            "    1.8535701  -0.46554533]\n",
            "  [-0.11377253  1.8289745   0.57534724 -0.9470324   0.16512568\n",
            "   -1.2494487  -1.5690728  -0.9428029   1.2746742   0.16968486\n",
            "    0.8942368  -0.08591411]\n",
            "  [-0.20253704  1.9508018   0.57432854 -0.48847356 -0.17936571\n",
            "   -1.2295091  -0.49982354 -1.2746893   0.3629765  -0.9087081\n",
            "    1.7849115   0.11008766]\n",
            "  [ 0.13465086  2.2006679   0.13278465  0.00297572  0.8017698\n",
            "   -1.2914212  -1.699594   -0.99470115  0.47130483 -0.279778\n",
            "    0.7547722  -0.23343147]]], shape=(3, 10, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Block\n",
        "\n",
        "Let's build the **Decoder Block**. Everything we did to create the **encoder** block applies here. The major differences are that the **Decoder Block** has:\n",
        "1. a **Multi-Head Cross-Attention** layer which uses the encoder's outputs as the keys and values.\n",
        "\n",
        "2. an extra skip/residual connection along with an extra layer normalization step.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1WVT4SX49bnta4uscOTF4xrsxFI4PbPER\" width=\"500\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "1cYi48-g73zQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mhsa = MultiHeadSelfAttention(d_model, num_heads)\n",
        "    self.ffn = feed_forward_network(d_model, hidden_dim)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  # Note the decoder block takes two masks. One for the first MHSA, another for the second MHSA.\n",
        "  def call(self, encoder_output, target, training, decoder_mask, memory_mask):\n",
        "    mhsa_output1, attn_weights = self.mhsa(target, target, target, decoder_mask)\n",
        "    mhsa_output1 = self.dropout(mhsa_output1, training=training)\n",
        "    mhsa_output1 = self.layernorm(mhsa_output1 + target)\n",
        "\n",
        "    mhsa_output2, attn_weights = self.mhsa(mhsa_output1, encoder_output, encoder_output, memory_mask) # q, k, v\n",
        "    mhsa_output2 = self.dropout(mhsa_output2, training=training)\n",
        "    mhsa_output2 = self.layernorm(mhsa_output2 + mhsa_output1)\n",
        "\n",
        "    ffn_output = self.ffn(mhsa_output2)\n",
        "    ffn_output = self.dropout(ffn_output, training=training)\n",
        "    output = self.layernorm(ffn_output + mhsa_output2)\n",
        "\n",
        "    return output, attn_weights"
      ],
      "metadata": {
        "id": "bqKg2dQg3QNb"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder\n",
        "\n",
        "The decoder is almost the same as the encoder except:\n",
        "- it takes the encoder's output as part of its input,\n",
        "- it takes two masks: the decoder mask and memory mask."
      ],
      "metadata": {
        "id": "WUVA-gcl-jba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_blocks, d_model, num_head, hidden_dim, target_vocab_size, max_seq_len, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.max_seq_len = max_seq_len\n",
        "\n",
        "    self.token_embed = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, d_model)\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    \n",
        "    self.blocks = [DecoderBlock(d_model, num_heads, hidden_dim, dropout_rate) \n",
        "                   for _ in range(num_blocks)]\n",
        "\n",
        "  def call(self, encoder_output, target, training, decoder_mask, memory_mask):\n",
        "    token_embeds = self.token_embed(target)\n",
        "\n",
        "    # Generate position indices\n",
        "    num_pos = target.shape[0] * self.max_seq_len\n",
        "    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)\n",
        "    pos_idx = np.reshape(pos_idx, target.shape)\n",
        "\n",
        "    pos_embeds = self.pos_embed(pos_idx)\n",
        "\n",
        "    x = self.dropout(token_embeds + pos_embeds, training=training)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x, weights = block(encoder_output, x, training, decoder_mask, memory_mask)\n",
        "    \n",
        "    return x, weights"
      ],
      "metadata": {
        "id": "i42hsjNH9bCn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we try the decoder, let's cover the masks involved. The decoder takes two masks:\n",
        "\n",
        "The *decoder mask* which is a <u>combination of two masks</u>: one to account for the padding in target sequences, and the look-ahead mask. This mask is used in the decoder's **first** multi-head self-attention layer.\n",
        "\n",
        "The *memory mask* which is used in the decoder's **second** multi-head self-attention. The keys and values for this layer are going to be the encoder's output, and this mask will ensure the decoder doesn't attend to any encoder output which corresponds to padding."
      ],
      "metadata": {
        "id": "Cb-7SMiX8nih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose this is our batch of vectorized target input sequences for the decoder.\n",
        "# These values are just made up\n",
        "target_input_seqs = [\n",
        "    [1, 652, 723, 123, 62],\n",
        "    [1, 25,  98, 129, 248, 215, 359, 249],\n",
        "    [1, 2369, 1259, 125, 486],\n",
        "]"
      ],
      "metadata": {
        "id": "PqqFu3Fj0cQS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to pad out this batch so that all sequences within it are the same length\n",
        "padded_target_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(target_input_seqs, padding=\"post\")\n",
        "print(\"Padded target inputs to the decoder:\")\n",
        "print(padded_target_input_seqs.shape)\n",
        "print(padded_target_input_seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcMgyynp0cNp",
        "outputId": "042a1308-a110-4935-f94f-f04cac39e34a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded target inputs to the decoder:\n",
            "(3, 8)\n",
            "[[   1  652  723  123   62    0    0    0]\n",
            " [   1   25   98  129  248  215  359  249]\n",
            " [   1 2369 1259  125  486    0    0    0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can create the padding mask the same way we did for the encoder\n",
        "dec_padding_mask = tf.cast(tf.math.not_equal(padded_target_input_seqs, 0), tf.float32)\n",
        "dec_padding_mask = dec_padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "print(dec_padding_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7DJqnbk0cKq",
        "outputId": "498e4597-babf-4f18-b6d2-c863e5b308eb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[1. 1. 1. 1. 1. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
            "\n",
            "\n",
            " [[[1. 1. 1. 1. 1. 0. 0. 0.]]]], shape=(3, 1, 1, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the look-ahead mask is a diagonal where the lower half are 1s and the upper half are zeros. This is easy to create using the *band_part* method:<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/linalg/band_part"
      ],
      "metadata": {
        "id": "8ya1y6FZ9QiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_input_seq_len = padded_target_input_seqs.shape[1]\n",
        "look_ahead_mask = tf.linalg.band_part(tf.ones((target_input_seq_len, \n",
        "                                               target_input_seq_len)), -1, 0)\n",
        "print(look_ahead_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBvcJ7479RIW",
        "outputId": "7ca6fe36-1eff-4029-c715-cda0219d0ee4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 1.]], shape=(8, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the decoder mask, we just need to combine the padding and look-ahead masks. Note how the columns of the resulting decoder mask are all zero for padding positions."
      ],
      "metadata": {
        "id": "KWuZxEHA9ozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dec_mask = tf.minimum(dec_padding_mask, look_ahead_mask)\n",
        "print(\"The decoder mask:\")\n",
        "print(dec_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u0TfHJO9pVB",
        "outputId": "834a867f-724e-47bc-eaa7-531e80d6b7ee"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The decoder mask:\n",
            "tf.Tensor(\n",
            "[[[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 1. 0.]\n",
            "   [1. 1. 1. 1. 1. 1. 1. 1.]]]\n",
            "\n",
            "\n",
            " [[[1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 0. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 0. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]\n",
            "   [1. 1. 1. 1. 1. 0. 0. 0.]]]], shape=(3, 1, 8, 8), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now declare a decoder and pass it everything it needs. In our case, the *memory* mask is the same as the *encoder* mask."
      ],
      "metadata": {
        "id": "x9G0_mOm93gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# num_blocks, d_model, num_head, hidden_dim, target_vocab_size, max_seq_len\n",
        "decoder = Decoder(6, 12, 4, 48, 10_000, 8)\n",
        "\n",
        "# encoder_output, target, training, decoder_mask, memory_mask\n",
        "decoder_output, _ = decoder(encoder_output, padded_target_input_seqs,\n",
        "                            True, dec_mask, enc_mask)\n",
        "\n",
        "print(f\"Decoder output {decoder_output.shape}:\")\n",
        "print(decoder_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZavzFSBY9RFs",
        "outputId": "c3a5d83b-0b9e-466b-a47d-9a5d217e4a12"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output (3, 8, 12):\n",
            "tf.Tensor(\n",
            "[[[-1.0184541   0.26625583  0.11572924 -1.1485747  -0.952984\n",
            "    0.8801752  -0.726415    0.8536025  -0.21449092  0.8939749\n",
            "   -1.0893704   2.140551  ]\n",
            "  [-0.9775529   0.8717778   0.32277203 -1.5117606   0.04323752\n",
            "    1.0193734  -1.1903638   1.3521185  -0.19306883  1.0657094\n",
            "   -1.4271342   0.62489164]\n",
            "  [-0.30989093  0.7311962   0.5223486  -1.8963698  -0.28913856\n",
            "    0.06362743 -0.8837329   1.0147457  -0.18362232  0.68921673\n",
            "   -1.3008653   1.8424853 ]\n",
            "  [-0.8711018   0.6517269   0.34115294 -1.5863849  -0.3171342\n",
            "    0.462965   -0.8037953   0.8512697  -0.05725691  0.7320541\n",
            "   -1.4009932   1.9974973 ]\n",
            "  [-0.9566183   0.5102372   0.30296198 -1.8084775  -0.21189342\n",
            "    0.47808024 -0.8638531   0.833414    0.08054852  0.63364947\n",
            "   -1.0534005   2.0553513 ]\n",
            "  [-1.0522851   0.70569533  0.04101041 -1.3774719  -0.35612231\n",
            "    0.22495836 -0.68459797  0.9391469  -0.07410909  0.7755334\n",
            "   -1.3008506   2.159093  ]\n",
            "  [-1.5343648   0.9857499   0.5224991  -1.5643884  -0.48459592\n",
            "    0.65294945 -0.80361074  1.1043572  -0.20314857  0.50095385\n",
            "   -0.768952    1.5925506 ]\n",
            "  [-1.2045605   0.3219039   0.32607195 -1.7588929  -0.02768933\n",
            "    0.48664472 -0.7046867   0.9801958  -0.02400544  0.5698789\n",
            "   -1.0126743   2.0478141 ]]\n",
            "\n",
            " [[-0.6326962   0.29047814  0.3470892  -1.4049395  -0.18380027\n",
            "    0.8285234  -1.0372022   0.4737067  -0.09163322  0.7578226\n",
            "   -1.4902567   2.1429076 ]\n",
            "  [-1.4664608   0.1894104   0.3796368  -1.4522564   0.7514693\n",
            "    0.80283934 -0.84489125  1.0629076  -0.70915926  0.3933721\n",
            "   -0.86569685  1.758829  ]\n",
            "  [-1.0583562   0.09927983 -0.12513627 -1.3107424  -0.33932248\n",
            "    0.8922461  -0.8134647   0.9555146  -0.40645474  0.64293855\n",
            "   -0.84873885  2.312236  ]\n",
            "  [-1.2907224   0.24058999  0.08916054 -1.3740426  -0.2676879\n",
            "    1.0033346  -0.7590085   1.1231371  -0.95776236  0.2965363\n",
            "   -0.20395045  2.1004152 ]\n",
            "  [-0.9168941   0.5503906   0.45529976 -1.7343664  -0.58179706\n",
            "    0.83554965 -0.53850865  0.4204017  -0.42279908  0.62124264\n",
            "   -0.87839514  2.1898758 ]\n",
            "  [-1.2249197   0.7448768   0.01356387 -1.7850672  -0.08556446\n",
            "    0.52121204 -0.27659968  0.95435375 -0.51806605  0.57497334\n",
            "   -0.9258035   2.0070405 ]\n",
            "  [-1.1913577   0.49024177  0.23022121 -1.551509   -0.04821522\n",
            "    0.911426   -0.88202393  1.2013346  -0.68900436  0.43383422\n",
            "   -0.786229    1.8812814 ]\n",
            "  [-0.74646217  0.34148192  0.600388   -1.4651768  -0.9122575\n",
            "    0.7725212  -1.0254833   0.66877586  0.26557603  0.00683617\n",
            "   -0.78597367  2.279774  ]]\n",
            "\n",
            " [[-1.4367379   0.7109173  -0.0370136  -1.403243    0.23385797\n",
            "    1.1459277  -0.9677636   1.1910454  -1.018718    0.5645839\n",
            "   -0.4550516   1.4721954 ]\n",
            "  [-1.2332242   0.58674526  0.34741086 -1.3998139  -0.713751\n",
            "    0.6862664  -0.80003726  1.1361148  -0.4591055  -0.03188447\n",
            "   -0.31544882  2.1967278 ]\n",
            "  [-1.4300637   0.814758    0.31596625 -1.4360656   0.88186836\n",
            "    0.4931753  -0.9292679   1.037801   -1.0547303   0.15664783\n",
            "   -0.51197827  1.6618886 ]\n",
            "  [-0.9129816   0.30193987  0.14503515 -1.4394755  -0.8253662\n",
            "    1.3053718  -1.1102769   0.764106   -0.3993767   0.58889776\n",
            "   -0.43728074  2.0194068 ]\n",
            "  [-1.5667446   0.9426436   0.20748056 -1.2559246   0.85867834\n",
            "    0.66514707 -0.99023527  0.7747841  -1.1317866   0.29655153\n",
            "   -0.44473153  1.6441375 ]\n",
            "  [-1.3472825   0.49792928  0.0802103  -1.4146687  -0.804697\n",
            "    1.1139728  -0.6430878   0.98561674 -0.60457146  0.63230807\n",
            "   -0.42105827  1.9253287 ]\n",
            "  [-0.93253726  0.8065498   0.34729436 -1.7530701   0.09306763\n",
            "    0.82874185 -0.92312115  0.7461872  -1.1857501   0.3810467\n",
            "   -0.29002994  1.8816212 ]\n",
            "  [-1.8429866   1.0279958   0.28266153 -1.3824763   0.48202217\n",
            "    1.0446575  -0.5810164   0.44265974 -1.0599514   0.2572622\n",
            "   -0.235067    1.5642388 ]]], shape=(3, 8, 12), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer\n",
        "\n",
        "We now have all the pieces to build the **Transformer** itself, and it's pretty simple. "
      ],
      "metadata": {
        "id": "ugdiBQ8q--oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_blocks, d_model, num_heads, hidden_dim,\n",
        "               source_vocab_size, target_vocab_size,\n",
        "               max_input_len, max_target_len, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_blocks, d_model, num_heads, hidden_dim, \n",
        "                           source_vocab_size, max_input_len, dropout_rate)\n",
        "    self.decoder = Decoder(num_blocks, d_model, num_heads, hidden_dim, \n",
        "                           target_vocab_size, max_target_len, dropout_rate)\n",
        "    \n",
        "    # The final dense layer to generate logits from the decoder output.\n",
        "    self.output_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, input_seqs, target_input_seqs, training, encoder_mask,\n",
        "           decoder_mask, memory_mask):\n",
        "    \n",
        "    encoder_output, encoder_attn_weights = self.encoder(input_seqs, training, encoder_mask)\n",
        "    decoder_output, decoder_attn_weights = self.decoder(encoder_output, \n",
        "                                                        target_input_seqs, training,\n",
        "                                                        decoder_mask, memory_mask)\n",
        "    \n",
        "    return self.output_layer(decoder_output), encoder_attn_weights, decoder_attn_weights"
      ],
      "metadata": {
        "id": "7Cos0Nyb9RC7"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(\n",
        "    num_blocks = 6, \n",
        "    d_model = 12, \n",
        "    num_heads = 3, \n",
        "    hidden_dim = 48,\n",
        "    source_vocab_size = bpemb_vocab_size, \n",
        "    target_vocab_size = 7000, # made-up target vocab size.\n",
        "    max_input_len = padded_input_seqs.shape[1],\n",
        "    max_target_len = padded_target_input_seqs.shape[1]\n",
        ")\n",
        "\n",
        "transformer_output, _, _ = transformer(padded_input_seqs, \n",
        "                                       padded_target_input_seqs,\n",
        "                                       True,\n",
        "                                       enc_mask,\n",
        "                                       dec_mask,\n",
        "                                       memory_mask=enc_mask)\n",
        "\n",
        "print(f\"Transformer output {transformer_output.shape}:\")\n",
        "print(transformer_output) # If training, we would use this output to calculate losses."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z02Rqk5n0cGs",
        "outputId": "0957a7b8-ba33-4032-e743-e2dc96427ebf"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer output (3, 8, 7000):\n",
            "tf.Tensor(\n",
            "[[[ 0.05081141 -0.01976041 -0.08560605 ... -0.04284935  0.01160289\n",
            "    0.09640501]\n",
            "  [ 0.03294214 -0.02145961 -0.08685193 ... -0.0535071  -0.00227882\n",
            "    0.06122992]\n",
            "  [-0.02347834 -0.0445135  -0.06411856 ... -0.04939048 -0.00934818\n",
            "    0.02314242]\n",
            "  ...\n",
            "  [-0.00267017 -0.03308434 -0.06233474 ... -0.06370658 -0.01462648\n",
            "    0.0490629 ]\n",
            "  [ 0.0129272  -0.01463469 -0.09329622 ... -0.02132286  0.01828239\n",
            "    0.01303657]\n",
            "  [-0.03911912 -0.04686867 -0.07727265 ... -0.05215853 -0.01322378\n",
            "   -0.00205435]]\n",
            "\n",
            " [[-0.06971866 -0.0376696  -0.06355062 ... -0.0368159   0.01256143\n",
            "   -0.07938278]\n",
            "  [-0.05778301 -0.05025372 -0.04995818 ... -0.05706041  0.00812414\n",
            "   -0.03802201]\n",
            "  [-0.02634483 -0.00221239 -0.07389838 ... -0.04657676 -0.04060621\n",
            "    0.01504142]\n",
            "  ...\n",
            "  [-0.03229667 -0.01470375 -0.07107651 ... -0.06028854  0.00238242\n",
            "   -0.04613585]\n",
            "  [-0.03567777 -0.05639028 -0.03368648 ... -0.07142429  0.02247666\n",
            "   -0.02796957]\n",
            "  [-0.04204955 -0.04404774 -0.04687225 ... -0.0615311   0.03388089\n",
            "   -0.03957418]]\n",
            "\n",
            " [[-0.05526959 -0.0443374  -0.07638802 ... -0.04624773  0.00774319\n",
            "   -0.05080967]\n",
            "  [-0.03600406 -0.05848981 -0.04440234 ... -0.07021327  0.01591255\n",
            "   -0.02755294]\n",
            "  [-0.0473726  -0.04796312 -0.06621274 ... -0.0675781   0.0013966\n",
            "   -0.03788979]\n",
            "  ...\n",
            "  [-0.0635036  -0.05080437 -0.05482534 ... -0.06373815 -0.01940861\n",
            "   -0.01879875]\n",
            "  [-0.04161209 -0.0558756  -0.06517394 ... -0.07357241  0.01419036\n",
            "   -0.03865527]\n",
            "  [-0.0669593  -0.03328177 -0.08215982 ... -0.03509401 -0.00286395\n",
            "   -0.06692656]]], shape=(3, 8, 7000), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's the whole original transformer from scratch. From here, if you want to train this transformer, Remember to use a learning rate warmup (Refer to the paper for more information on this).<br><br>\n",
        "\n",
        "It's useful to know how these models work under the hood, but to train our own transformer to get impressive results is expensive. Both in terms of compute and data.<br>\n",
        "\n",
        "Fortunately, there's a zoo of **pretrained** transformer models we can use. "
      ],
      "metadata": {
        "id": "43ydlCqIB2de"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Papers**<br>\n",
        "Attention Is All You Need (original Transformer paper): https://arxiv.org/abs/1706.03762<br>\n",
        "\n",
        "The Annotated Transformer: http://nlp.seas.harvard.edu/annotated-transformer/<br>\n",
        "\n",
        "GPT-3: https://arxiv.org/abs/2005.14165<br>\n",
        "\n",
        "BERT: https://arxiv.org/abs/1810.04805<br>\n",
        "\n",
        "RoBERTa paper: https://arxiv.org/abs/1907.11692<br>\n",
        "\n",
        "ALBERT paper: https://arxiv.org/abs/1909.11942<br>\n",
        "\n",
        "DistilBert paper: https://arxiv.org/abs/1910.01108<br>\n",
        "\n",
        "Electra paper: https://arxiv.org/abs/2003.10555<br>\n",
        "\n",
        "XLM: https://arxiv.org/abs/1901.07291<br>"
      ],
      "metadata": {
        "id": "2GDMeu9FaGLc"
      }
    }
  ]
}