{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPjXy7fqbhSr"
      },
      "source": [
        "We'll build a language model trained on the Art of War by Sun Tzu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoALMeX7bmPH"
      },
      "source": [
        "The language model we'll build will be **Word**-based (as opposed to charachter-based). That is, given a sequence of one or more characters, the model will be asked to predict the next character.<br><br>\n",
        "\n",
        "A word-level language model is a statistical language model that predicts the probability of a word given the preceding words in a sequence. \n",
        "\n",
        "Word-level models are preferred over character-level models because they have a smaller vocabulary and display higher accuracy and lower computational cost than char-based models. \n",
        "\n",
        "Word-level models can use a distributed representation where different words with similar meanings have similar representation and can use a large context of recently observed words when making predictions.\n",
        "\n",
        "\n",
        "\n",
        "RNNs can process any kind of sequence so what's shown here can easily be applied at the Char level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duK_zHLMm1js"
      },
      "source": [
        "# Steps\n",
        "- Load the dataset\n",
        "- Prepare the dataset for modeling\n",
        "- Define the model architecture\n",
        "- Train the model\n",
        "- Generate text using the trained model\n",
        "\n",
        "Let's get started!\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrTUZg4kmroz",
        "outputId": "dc9b9b15-bc46-48c9-8546-3aaa34a24df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Sun Tzŭ said: The art of war is of vital importance to the State.\n",
            "\n",
            "2. It is a matter of life and \n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "# Download the art of war dataset\n",
        "url = \"https://raw.githubusercontent.com/jrreda/AI-projects/main/Language%20Modelling/art_of_war.txt\"\n",
        "text = requests.get(url).text\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhmJl28JndlM"
      },
      "source": [
        "# Step 2: Prepare the dataset for modeling\n",
        "\n",
        "Next, you need to prepare the text dataset for modeling. In this step, we'll do the following:\n",
        "\n",
        "- Tokenize the text into words\n",
        "- Convert the words to lowercase\n",
        "- Create sequences of words to use as input/output pairs for the model\n",
        "- Encode the words as integers\n",
        "\n",
        "Here's the code to do these steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Hfg01yPgmrt5"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "# clean the text data\n",
        "text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
        "text = text.lower() # convert to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "anXPuoXcmrrS"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# tokenize the text into words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "sequences = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "# create a dictionary that maps words to integers\n",
        "word_index = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VlbzS7rHmrwn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# encode the sequences of words as integers\n",
        "vocab_size = len(word_index) + 1\n",
        "encoded = np.array(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KHLChAcNpleR"
      },
      "outputs": [],
      "source": [
        "# create sequences of words for training the model\n",
        "seq_len = 128\n",
        "sequences = []\n",
        "for i in range(seq_len, len(encoded)):\n",
        "  sequence = encoded[i-seq_len: i+1]\n",
        "  sequences.append(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br7XyVQHp0-h",
        "outputId": "f678032f-f84a-4b5f-9775-0aa4b4acf241"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 66,  87, 104, 105,   1,  98,   2,  70,   5,   2, 962, 963,   3,\n",
              "         1, 170,  67,  11,   5,   6, 620,   2, 446,   4, 171,   6, 447,\n",
              "       356,   3, 357,  54,   3, 448,  56,  11,   5,   6, 449,   2, 964,\n",
              "        30,  31,  17,  49, 229,   8, 965,  68,   1,  98,   2,  70,  71,\n",
              "         5, 966,  22,  75, 450, 967,   3,   8, 621,  57, 229,   7, 194,\n",
              "       622,  20, 623,   3, 624,   1, 279, 968,   7,   1, 172,  72,  76,\n",
              "        10,  66,   1, 358, 359,  67, 230,  68, 155,  72,   1, 625,  73,\n",
              "       231,   4, 195,  73,  88,   1, 358, 359, 232,   1, 360,   3,   8,\n",
              "         7, 280, 626,  23,  42, 196,  77,  21,  38,   9, 361,  43, 969,\n",
              "         2,  42, 451, 970,  22, 120, 362,  94, 230, 971, 281,   4])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "sequences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Iwdu6fGEqVjg"
      },
      "outputs": [],
      "source": [
        "# pad sequences to make them all the same length\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "max_len = max([len(seq) for seq in sequences]) # len(max(sequences, key=len))\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pf3_yD8r8Ui",
        "outputId": "ced516e4-a966-45b5-91ab-95e82ab29720"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 66,  87, 104, 105,   1,  98,   2,  70,   5,   2, 962, 963,   3,\n",
              "          1, 170,  67,  11,   5,   6, 620,   2, 446,   4, 171,   6, 447,\n",
              "        356,   3, 357,  54,   3, 448,  56,  11,   5,   6, 449,   2, 964,\n",
              "         30,  31,  17,  49, 229,   8, 965,  68,   1,  98,   2,  70,  71,\n",
              "          5, 966,  22,  75, 450, 967,   3,   8, 621,  57, 229,   7, 194,\n",
              "        622,  20, 623,   3, 624,   1, 279, 968,   7,   1, 172,  72,  76,\n",
              "         10,  66,   1, 358, 359,  67, 230,  68, 155,  72,   1, 625,  73,\n",
              "        231,   4, 195,  73,  88,   1, 358, 359, 232,   1, 360,   3,   8,\n",
              "          7, 280, 626,  23,  42, 196,  77,  21,  38,   9, 361,  43, 969,\n",
              "          2,  42, 451, 970,  22, 120, 362,  94, 230, 971, 281], dtype=int32),\n",
              " array([0., 0., 0., ..., 0., 0., 0.], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "X[0], y[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0Fw1GxKcjTr"
      },
      "source": [
        "`BUFFER_SIZE` is a parameter that controls how much data is shuffled before batching. When I created the training dataset from the pairs of input and output sequences, I used the shuffle method to randomize the order of the pairs. This helps to avoid overfitting and improve generalization. However, shuffling the entire dataset at once can be very expensive and slow. Therefore, I used a `BUFFER_SIZE` to specify how many pairs to shuffle at a time. For example, if I have 10000 pairs and I use a `BUFFER_SIZE` of 1000, the shuffle method will take 1000 pairs at a time and shuffle them randomly, then take another 1000 pairs and shuffle them, and so on. This way, I can shuffle the data faster and more efficiently.\n",
        "<br><br>\n",
        "`.prefetch` is a method that allows the dataset to fetch the next batch of data while the model is training on the current batch. This helps to reduce the idle time of the model and improve the training speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# define batch size and buffer size\n",
        "batch_size = 32\n",
        "buffer_size = 10000\n",
        "\n",
        "# create dataset from X and y\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "\n",
        "# shuffle the dataset and split into batches\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "mVWX0GCrROt3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UpUjTahRYit",
        "outputId": "f1ff5bfa-a39a-4d90-bef5-c1d2c8591332"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), TensorSpec(shape=(32, 2232), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I-CalsasSFl"
      },
      "source": [
        "# Step 3: Define the model architecture\n",
        "\n",
        "Now that you have your dataset prepared, you can define your language model. In this example, we'll use a simple feedforward neural network with an embedding layer, a dense hidden layer, and a softmax output layer.\n",
        "\n",
        "Here's the code to define the model:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download glove embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gejl0I5Mci0W",
        "outputId": "96acbb38-08db-448d-d31c-1a56b2b1a942"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-27 14:26:05--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-03-27 14:26:05--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-03-27 14:26:05--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.94MB/s    in 2m 39s  \n",
            "\n",
            "2023-03-27 14:28:44 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the GloVe word vectors\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "# create an embedding matrix\n",
        "embedding_dim = 100\n",
        "num_words = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "HyHvsF36d2-C"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_3kxpxgsT4R",
        "outputId": "5cc5b280-c623-4d82-8f0c-241655960c2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 128, 100)          223200    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 128, 100)          0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128, 256)         234496    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 128, 256)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 128, 256)         394240    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 128, 256)          0         \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 128, 256)         394240    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 128)               197120    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2232)              287928    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,731,224\n",
            "Trainable params: 1,731,224\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "# recurrent_dropout=0 to support cuDNN \n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len-1),\n",
        "    Dropout(0.3),\n",
        "    Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0)),  \n",
        "    Dropout(0.3),\n",
        "    Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0)),  \n",
        "    Dropout(0.3),\n",
        "    Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0)),  \n",
        "    LSTM(128, recurrent_dropout=0),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.utils import plot_model\n",
        "\n",
        "# plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "pYpW8epOPj8i"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "XaqSmkFIsttx"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.002, weight_decay=0.0001))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUXhrUOLtg35"
      },
      "source": [
        "Step 4: Train the model\n",
        "\n",
        "Now that you have your model defined, you can train it on the input/output pairs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_Ksy1Yhs-ZR",
        "outputId": "05bd1dcb-528e-476b-8f0c-03d9409cc701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "335/335 [==============================] - 41s 86ms/step - loss: 6.5100\n",
            "Epoch 2/50\n",
            "335/335 [==============================] - 18s 55ms/step - loss: 6.1131\n",
            "Epoch 3/50\n",
            "335/335 [==============================] - 15s 43ms/step - loss: 5.9259\n",
            "Epoch 4/50\n",
            "335/335 [==============================] - 15s 43ms/step - loss: 5.7412\n",
            "Epoch 5/50\n",
            "335/335 [==============================] - 14s 42ms/step - loss: 5.5822\n",
            "Epoch 6/50\n",
            "335/335 [==============================] - 15s 45ms/step - loss: 5.4447\n",
            "Epoch 7/50\n",
            "335/335 [==============================] - 20s 61ms/step - loss: 5.3142\n",
            "Epoch 8/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 5.1915\n",
            "Epoch 9/50\n",
            "335/335 [==============================] - 13s 40ms/step - loss: 5.0672\n",
            "Epoch 10/50\n",
            "335/335 [==============================] - 17s 51ms/step - loss: 4.9542\n",
            "Epoch 11/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 4.8413\n",
            "Epoch 12/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 4.7343\n",
            "Epoch 13/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 4.6395\n",
            "Epoch 14/50\n",
            "335/335 [==============================] - 18s 54ms/step - loss: 4.5480\n",
            "Epoch 15/50\n",
            "335/335 [==============================] - 15s 44ms/step - loss: 4.4521\n",
            "Epoch 16/50\n",
            "335/335 [==============================] - 13s 40ms/step - loss: 4.3640\n",
            "Epoch 17/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 4.2641\n",
            "Epoch 18/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 4.1676\n",
            "Epoch 19/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 4.0768\n",
            "Epoch 20/50\n",
            "335/335 [==============================] - 15s 44ms/step - loss: 3.9842\n",
            "Epoch 21/50\n",
            "335/335 [==============================] - 20s 59ms/step - loss: 3.8849\n",
            "Epoch 22/50\n",
            "335/335 [==============================] - 14s 42ms/step - loss: 3.7903\n",
            "Epoch 23/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 3.6922\n",
            "Epoch 24/50\n",
            "335/335 [==============================] - 13s 38ms/step - loss: 3.6112\n",
            "Epoch 25/50\n",
            "335/335 [==============================] - 13s 38ms/step - loss: 3.5305\n",
            "Epoch 26/50\n",
            "335/335 [==============================] - 13s 38ms/step - loss: 3.4384\n",
            "Epoch 27/50\n",
            "335/335 [==============================] - 13s 38ms/step - loss: 3.3490\n",
            "Epoch 28/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 3.2639\n",
            "Epoch 29/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 3.1960\n",
            "Epoch 30/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 3.0921\n",
            "Epoch 31/50\n",
            "335/335 [==============================] - 13s 38ms/step - loss: 3.0224\n",
            "Epoch 32/50\n",
            "335/335 [==============================] - 13s 38ms/step - loss: 2.9517\n",
            "Epoch 33/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 2.8731\n",
            "Epoch 34/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 2.8112\n",
            "Epoch 35/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 2.7241\n",
            "Epoch 36/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 2.6573\n",
            "Epoch 37/50\n",
            "335/335 [==============================] - 15s 44ms/step - loss: 2.5911\n",
            "Epoch 38/50\n",
            "335/335 [==============================] - 18s 53ms/step - loss: 2.5243\n",
            "Epoch 39/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 2.4710\n",
            "Epoch 40/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 2.4100\n",
            "Epoch 41/50\n",
            "335/335 [==============================] - 13s 38ms/step - loss: 2.3402\n",
            "Epoch 42/50\n",
            "335/335 [==============================] - 13s 38ms/step - loss: 2.2824\n",
            "Epoch 43/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 2.2324\n",
            "Epoch 44/50\n",
            "335/335 [==============================] - 13s 38ms/step - loss: 2.1693\n",
            "Epoch 45/50\n",
            "335/335 [==============================] - 13s 39ms/step - loss: 2.1165\n",
            "Epoch 46/50\n",
            "335/335 [==============================] - 13s 38ms/step - loss: 2.0539\n",
            "Epoch 47/50\n",
            "335/335 [==============================] - 13s 38ms/step - loss: 2.0103\n",
            "Epoch 48/50\n",
            "335/335 [==============================] - 13s 38ms/step - loss: 1.9650\n",
            "Epoch 49/50\n",
            "335/335 [==============================] - 13s 38ms/step - loss: 1.9487\n",
            "Epoch 50/50\n",
            "335/335 [==============================] - 13s 38ms/step - loss: 1.8704\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc3c4790850>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "# model.fit(X, y, epochs=20, verbose=1)\n",
        "model.fit(dataset, epochs=50, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "model.save('art_of_war_word_level_100embed_50epochs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKRjoQrzg-rv",
        "outputId": "dc5ffc6a-0a6e-4dfe-d8b1-c1d38e982c77"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_fn, lstm_cell_11_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved model\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('art_of_war_word_level_100embed_50epochs')"
      ],
      "metadata": {
        "id": "1rHa5DZ7hTJC"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgrHxI0WtaIW"
      },
      "source": [
        "# Step 5: Generate text using the trained model\n",
        "\n",
        "Finally, you can use the trained model to generate text. To do this, you'll start with a seed word and generate the next word in the sequence using the model. You'll continue this process, generating one word at a time, until you've generated the desired amount of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "rodhFkajtAFR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def generate_text(seed_text, length=10, temperature=1):\n",
        "\n",
        "  text = seed_text  \n",
        "\n",
        "  for _ in range(length):\n",
        "\n",
        "    # Take the last *seq_len* number of characters in the text so far as input.\n",
        "    sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    sequence = pad_sequences([sequence], maxlen=max_len-1, padding='pre')\n",
        "\n",
        "    # Create probability distribution for next character adjusted by temperature.\n",
        "    preds = model.predict(sequence, verbose=0) # <-- We want only the last character so we're extracting the softmax output for that.\n",
        "    preds = tf.math.log(preds) / temperature\n",
        "\n",
        "    # Sample next character and add to running text.\n",
        "    next_word = tf.random.categorical(preds, num_samples=1)\n",
        "    next_word = tokenizer.sequences_to_texts(next_word.numpy())[0]\n",
        "    # print(next_word)\n",
        "\n",
        "    text += next_word\n",
        "    text += ' '\n",
        "\n",
        "  return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Banana peels on the battlefield can \", length=30, temperature=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w-311yg8TB4l",
        "outputId": "ae1dae0b-4762-42cf-92ef-f7b607e38eb1"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Banana peels on the battlefield can ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever ever'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"It's time to release the Kraken when \", length=30, temperature=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nZH6kY30Y4XV",
        "outputId": "44e1b49c-c7e5-41a1-96bc-d307f521d12d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It's time to release the Kraken when wing the wing wing will will will the will the you wing wing wing wing will wing the wing will you wing wing wing wing wing wing will wing will\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Crush your enemies, see them driven before you, and \", length=30, temperature=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "feS3PC2ybose",
        "outputId": "b4e98372-38e7-4f71-e39c-d18a74a18aca"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Crush your enemies, see them driven before you, and penetrated force gone penetrated penetrated the situated grown get fallen in fallen fallen fallen fallen in gone grown gone to penetrated hold heard a still penetrated penetrated leading grown fallen'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"What is best in life? \", length=30, temperature=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LWwSxNVrbjf4",
        "outputId": "54c678f4-27c3-4984-cc2f-923ba8c48aad"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is best in life? nor without 35 1 10 on 28 so in and 14 17 and and but 13 then strike attack 18 on foe strike i if 14 may such where so'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast to our language model, GPT-3 has 175 billion parameters and was trained on 45 terabytes of data, but the high-level principle of learning through prediction remains the same."
      ],
      "metadata": {
        "id": "Hh3obtH3qQMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using a pre-trained language model like GPT-2\n",
        "\n",
        "GPT-2 has 1.5B parameters"
      ],
      "metadata": {
        "id": "cWJw96eim7Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "pOfB-1Irm_nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "h8PBunJabx_V"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "UoO-MNkwm-Ol"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to generate text\n",
        "gpt2_model.eval() "
      ],
      "metadata": {
        "id": "SvRu76tWnhG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "input_text = \"Crush your enemies, see them driven before you, and \"\n",
        "input_ids = gpt2_tokenizer.encode(input_text, return_tensors='pt')\n",
        "output = gpt2_model.generate(input_ids, max_length=250, do_sample=True, num_beams=5, temperature=1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiNAo9q7npoV",
        "outputId": "c40da1bf-4113-48a6-d7ba-57083f783b8d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the generated output\n",
        "generated_text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True).replace('\\xa0', ' ')\n",
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "dORRzd6QoPu1",
        "outputId": "8d305589-24a8-4dfa-a3a2-0f4a6ef59f56"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Crush your enemies, see them driven before you, and  make sure you don't let them get close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too close to you.  Don't let them get too\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ESQCjywgofcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HKH7FzXQtPH1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}