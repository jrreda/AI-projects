{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFEbw1OTQRwK"
      },
      "source": [
        "We'll build a language model trained on the Art of War by Sun Tzu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FITGWsS5QBxo"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DnJx_CfbTOKI",
        "outputId": "89410fe4-5c1f-446e-8a3b-af6862a34632"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1. Sun Tzŭ said: The art of war is of vital importance to the State.\\n\\n2. It is a matter of life and death, a road either to safety or to\\nruin. Hence it is a subject of inquiry which can on no account be\\nneglected.\\n\\n3. The art of war, then, is governed by five constant factors, to be\\ntaken into accou'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "art_of_war = requests.get('https://raw.githubusercontent.com/jrreda/AI-projects/main/Language%20Modelling/art_of_war.txt')\\\n",
        "                     .text\n",
        "\n",
        "art_of_war[:300]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvHOeEA_UgRC"
      },
      "source": [
        "The language model we'll build will be **character**-based (as opposed to word-based). That is, given a sequence of one or more characters, the model will be asked to predict the next character.<br><br>\n",
        "Character-level models have the advantage of:\n",
        "- Smaller prediction space. There are only a handful of characters in the English language compared to the tens of thousands of words in a typical corpus.\n",
        "- Character-level models are more resilient to out-of-vocabulary (OOV) conditions and are better able to learn the lower mechanics of language (including punctuation).<br><br>\n",
        "\n",
        "On the other hand, character-level models need to learn a sequence of characters to \"make sense\" of a word (e.g. the sequence of \"c\", \"a\", \"t\" to identify \"cat\" as a pattern) which can be inefficient and result in lower performance.<br><br>\n",
        "RNNs can process any kind of sequence so what's shown here can easily be applied at the word level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGCNLbjRXqMp"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "moPBGficT_7e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qc9gUjORTQ9X"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts([art_of_war])\n",
        "\n",
        "seq = tokenizer.texts_to_sequences([art_of_war])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7f6VEqdUeRF",
        "outputId": "73efc87a-15f5-4f23-ff67-093e3f988f6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_words': None,\n",
              " 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
              " 'lower': True,\n",
              " 'split': ' ',\n",
              " 'char_level': True,\n",
              " 'oov_token': None,\n",
              " 'document_count': 1,\n",
              " 'word_counts': '{\"1\": 179, \".\": 896, \" \": 9794, \"s\": 3081, \"u\": 1467, \"n\": 3565, \"t\": 4398, \"z\": 20, \"\\\\u016d\": 13, \"a\": 3475, \"i\": 3573, \"d\": 1681, \":\": 48, \"h\": 2558, \"e\": 5837, \"r\": 2776, \"o\": 3548, \"f\": 1238, \"w\": 981, \"v\": 478, \"l\": 1722, \"m\": 1201, \"p\": 769, \"c\": 1390, \"\\\\n\": 1443, \"2\": 127, \",\": 634, \"y\": 1055, \"b\": 708, \"j\": 23, \"q\": 55, \"g\": 1007, \"3\": 87, \"k\": 345, \"\\\\u2019\": 57, \"4\": 66, \"(\": 59, \")\": 59, \";\": 168, \"5\": 58, \"6\": 51, \"_\": 62, \"7\": 39, \"8\": 36, \"9\": 34, \"0\": 38, \"x\": 49, \"\\\\u2014\": 16, \"?\": 8, \"!\": 8, \"-\": 57, \"\\\\u201c\": 3, \"\\\\u201d\": 3, \"\\\\u0153\": 7, \"\\\\u00fc\": 3, \"\\\\u2018\": 1}',\n",
              " 'word_docs': '{\"\\\\u2019\": 1, \"!\": 1, \"9\": 1, \".\": 1, \"c\": 1, \"v\": 1, \"j\": 1, \";\": 1, \"-\": 1, \"0\": 1, \")\": 1, \"f\": 1, \"y\": 1, \"7\": 1, \"(\": 1, \"\\\\u201d\": 1, \"h\": 1, \"t\": 1, \"\\\\u016d\": 1, \"r\": 1, \"m\": 1, \"a\": 1, \"u\": 1, \"\\\\n\": 1, \"n\": 1, \"d\": 1, \"l\": 1, \"2\": 1, \"z\": 1, \"8\": 1, \" \": 1, \"\\\\u2018\": 1, \"w\": 1, \"q\": 1, \"k\": 1, \"p\": 1, \"\\\\u2014\": 1, \"_\": 1, \"\\\\u00fc\": 1, \"i\": 1, \":\": 1, \"e\": 1, \",\": 1, \"3\": 1, \"6\": 1, \"x\": 1, \"o\": 1, \"s\": 1, \"1\": 1, \"4\": 1, \"g\": 1, \"b\": 1, \"5\": 1, \"\\\\u0153\": 1, \"?\": 1, \"\\\\u201c\": 1}',\n",
              " 'index_docs': '{\"36\": 1, \"51\": 1, \"45\": 1, \"21\": 1, \"15\": 1, \"25\": 1, \"46\": 1, \"28\": 1, \"37\": 1, \"43\": 1, \"34\": 1, \"16\": 1, \"18\": 1, \"42\": 1, \"33\": 1, \"54\": 1, \"10\": 1, \"3\": 1, \"49\": 1, \"9\": 1, \"17\": 1, \"7\": 1, \"13\": 1, \"14\": 1, \"5\": 1, \"12\": 1, \"11\": 1, \"29\": 1, \"47\": 1, \"44\": 1, \"1\": 1, \"56\": 1, \"20\": 1, \"38\": 1, \"26\": 1, \"22\": 1, \"48\": 1, \"32\": 1, \"55\": 1, \"4\": 1, \"41\": 1, \"2\": 1, \"24\": 1, \"30\": 1, \"39\": 1, \"40\": 1, \"6\": 1, \"8\": 1, \"27\": 1, \"31\": 1, \"19\": 1, \"23\": 1, \"35\": 1, \"52\": 1, \"50\": 1, \"53\": 1}',\n",
              " 'index_word': '{\"1\": \" \", \"2\": \"e\", \"3\": \"t\", \"4\": \"i\", \"5\": \"n\", \"6\": \"o\", \"7\": \"a\", \"8\": \"s\", \"9\": \"r\", \"10\": \"h\", \"11\": \"l\", \"12\": \"d\", \"13\": \"u\", \"14\": \"\\\\n\", \"15\": \"c\", \"16\": \"f\", \"17\": \"m\", \"18\": \"y\", \"19\": \"g\", \"20\": \"w\", \"21\": \".\", \"22\": \"p\", \"23\": \"b\", \"24\": \",\", \"25\": \"v\", \"26\": \"k\", \"27\": \"1\", \"28\": \";\", \"29\": \"2\", \"30\": \"3\", \"31\": \"4\", \"32\": \"_\", \"33\": \"(\", \"34\": \")\", \"35\": \"5\", \"36\": \"\\\\u2019\", \"37\": \"-\", \"38\": \"q\", \"39\": \"6\", \"40\": \"x\", \"41\": \":\", \"42\": \"7\", \"43\": \"0\", \"44\": \"8\", \"45\": \"9\", \"46\": \"j\", \"47\": \"z\", \"48\": \"\\\\u2014\", \"49\": \"\\\\u016d\", \"50\": \"?\", \"51\": \"!\", \"52\": \"\\\\u0153\", \"53\": \"\\\\u201c\", \"54\": \"\\\\u201d\", \"55\": \"\\\\u00fc\", \"56\": \"\\\\u2018\"}',\n",
              " 'word_index': '{\" \": 1, \"e\": 2, \"t\": 3, \"i\": 4, \"n\": 5, \"o\": 6, \"a\": 7, \"s\": 8, \"r\": 9, \"h\": 10, \"l\": 11, \"d\": 12, \"u\": 13, \"\\\\n\": 14, \"c\": 15, \"f\": 16, \"m\": 17, \"y\": 18, \"g\": 19, \"w\": 20, \".\": 21, \"p\": 22, \"b\": 23, \",\": 24, \"v\": 25, \"k\": 26, \"1\": 27, \";\": 28, \"2\": 29, \"3\": 30, \"4\": 31, \"_\": 32, \"(\": 33, \")\": 34, \"5\": 35, \"\\\\u2019\": 36, \"-\": 37, \"q\": 38, \"6\": 39, \"x\": 40, \":\": 41, \"7\": 42, \"0\": 43, \"8\": 44, \"9\": 45, \"j\": 46, \"z\": 47, \"\\\\u2014\": 48, \"\\\\u016d\": 49, \"?\": 50, \"!\": 51, \"\\\\u0153\": 52, \"\\\\u201c\": 53, \"\\\\u201d\": 54, \"\\\\u00fc\": 55, \"\\\\u2018\": 56}'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tokenizer.get_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsAQXdN4XphD",
        "outputId": "4acbafc6-64f2-477b-b0fb-618de2ab6918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer \"Vocabulary\" size: 56\n"
          ]
        }
      ],
      "source": [
        "print(f'Tokenizer \"Vocabulary\" size: {len(tokenizer.word_index)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJDgsN63X0sX",
        "outputId": "3b09c650-5642-46b6-a429-b259746d2699"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1 .   s u n   t z ŭ']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Sanity check.\n",
        "tokenizer.sequences_to_texts([seq[:10]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr7WRUBuX_Ks"
      },
      "source": [
        "Our training data is currently one long sequence which we'll need to segment into training examples. To do this, we'll use the **Tensorflow Data** API which makes it easy to build preprocessing pipelines by chaining operations together.<br>\n",
        "https://www.tensorflow.org/guide/data<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/data<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3njrsJErX8e4",
        "outputId": "23192166-26f0-4361-ac76-9e6572d7ae4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.from_tensor_slices_op.TensorSliceDataset"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "slices = tf.data.Dataset.from_tensor_slices(seq)\n",
        "type(slices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ej10RnAYvii",
        "outputId": "bc3655d8-1cd1-4a72-82fa-c714b5c591a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<tf.Tensor: shape=(), dtype=int32, numpy=27>,\n",
              "  <tf.Tensor: shape=(), dtype=int32, numpy=21>,\n",
              "  <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
              "  <tf.Tensor: shape=(), dtype=int32, numpy=8>,\n",
              "  <tf.Tensor: shape=(), dtype=int32, numpy=13>,\n",
              "  <tf.Tensor: shape=(), dtype=int32, numpy=5>,\n",
              "  <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
              "  <tf.Tensor: shape=(), dtype=int32, numpy=3>,\n",
              "  <tf.Tensor: shape=(), dtype=int32, numpy=47>,\n",
              "  <tf.Tensor: shape=(), dtype=int32, numpy=49>],\n",
              " [27, 21, 1, 8, 13, 5, 1, 3, 47, 49])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "list(slices.take(10)), seq[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbE77McwZAm0"
      },
      "source": [
        "Here, we're creating windows of `input_timesteps + 1`. The *input_timesteps* represents our training example length. The *+1* is there to help us create the target/label for each training example. This will be clarified further below.<br><br>\n",
        "Finally, we're setting *drop_remainder* to True which ensures ALL windows contain exactly N elements. i.e. once the input contains fewer than N elements, they are ignored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w3_DNeOlY49J"
      },
      "outputs": [],
      "source": [
        "# create the training examples\n",
        "input_timesteps = 100\n",
        "window_size = input_timesteps + 1\n",
        "windows = slices.window(window_size, shift=1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsnsG7UtZeZo",
        "outputId": "16255d47-076f-423d-e2bb-7176958bd3e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101 [27, 21, 1, 8, 13, 5, 1, 3, 47, 49, 1, 8, 7, 4, 12, 41, 1, 3, 10, 2, 1, 7, 9, 3, 1, 6, 16, 1, 20, 7, 9, 1, 4, 8, 1, 6, 16, 1, 25, 4, 3, 7, 11, 1, 4, 17, 22, 6, 9, 3, 7, 5, 15, 2, 1, 3, 6, 1, 3, 10, 2, 1, 8, 3, 7, 3, 2, 21, 14, 14, 29, 21, 1, 4, 3, 1, 4, 8, 1, 7, 1, 17, 7, 3, 3, 2, 9, 1, 6, 16, 1, 11, 4, 16, 2, 1, 7, 5, 12, 1, 12]\n",
            "101 [21, 1, 8, 13, 5, 1, 3, 47, 49, 1, 8, 7, 4, 12, 41, 1, 3, 10, 2, 1, 7, 9, 3, 1, 6, 16, 1, 20, 7, 9, 1, 4, 8, 1, 6, 16, 1, 25, 4, 3, 7, 11, 1, 4, 17, 22, 6, 9, 3, 7, 5, 15, 2, 1, 3, 6, 1, 3, 10, 2, 1, 8, 3, 7, 3, 2, 21, 14, 14, 29, 21, 1, 4, 3, 1, 4, 8, 1, 7, 1, 17, 7, 3, 3, 2, 9, 1, 6, 16, 1, 11, 4, 16, 2, 1, 7, 5, 12, 1, 12, 2]\n",
            "101 [1, 8, 13, 5, 1, 3, 47, 49, 1, 8, 7, 4, 12, 41, 1, 3, 10, 2, 1, 7, 9, 3, 1, 6, 16, 1, 20, 7, 9, 1, 4, 8, 1, 6, 16, 1, 25, 4, 3, 7, 11, 1, 4, 17, 22, 6, 9, 3, 7, 5, 15, 2, 1, 3, 6, 1, 3, 10, 2, 1, 8, 3, 7, 3, 2, 21, 14, 14, 29, 21, 1, 4, 3, 1, 4, 8, 1, 7, 1, 17, 7, 3, 3, 2, 9, 1, 6, 16, 1, 11, 4, 16, 2, 1, 7, 5, 12, 1, 12, 2, 7]\n"
          ]
        }
      ],
      "source": [
        "# Sanity check.\n",
        "for t in windows.take(3):\n",
        "  arr = list(t.as_numpy_iterator())\n",
        "  print(len(arr), arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCsj7JYPZ9eT"
      },
      "source": [
        "The *window* method returns a nested dataset of datasets (i.e. each window is a dataset containing a tensor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC5cK0U2ZuIY",
        "outputId": "b0636c1f-e067-4a99-f5dd-043b992a79c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<WindowDataset element_spec=DatasetSpec(TensorSpec(shape=(), dtype=tf.int32, name=None), TensorShape([]))> \n",
            "\n",
            "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n",
            "<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n"
          ]
        }
      ],
      "source": [
        "print(windows, '\\n')\n",
        "\n",
        "for w in windows.take(2):\n",
        "  print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXrSbBThZ-jx"
      },
      "source": [
        "But our model won't accept those. It'll accept only tensors, so we need to extract the tensors from each window. To do that, we'll use *flat_map* which will flatten the dataset of datasets into a single dataset of elements. But because we want to retain our segmented sequences, we'll also pass in a *batch* function to maintain the segments (otherwise, we'll just get back one large tensor representing our whole corpus).<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYz94hDxZ-0w",
        "outputId": "38e2ab02-199e-4545-84c3-ca0844adedb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "dataset = windows.flat_map(lambda window: window.batch(window_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjn3ZqhQaBmG",
        "outputId": "8d0b89be-779b-43c5-d6fc-a3c507e82c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[27 21  1  8 13  5  1  3 47 49  1  8  7  4 12 41  1  3 10  2  1  7  9  3\n",
            "  1  6 16  1 20  7  9  1  4  8  1  6 16  1 25  4  3  7 11  1  4 17 22  6\n",
            "  9  3  7  5 15  2  1  3  6  1  3 10  2  1  8  3  7  3  2 21 14 14 29 21\n",
            "  1  4  3  1  4  8  1  7  1 17  7  3  3  2  9  1  6 16  1 11  4 16  2  1\n",
            "  7  5 12  1 12], shape=(101,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[21  1  8 13  5  1  3 47 49  1  8  7  4 12 41  1  3 10  2  1  7  9  3  1\n",
            "  6 16  1 20  7  9  1  4  8  1  6 16  1 25  4  3  7 11  1  4 17 22  6  9\n",
            "  3  7  5 15  2  1  3  6  1  3 10  2  1  8  3  7  3  2 21 14 14 29 21  1\n",
            "  4  3  1  4  8  1  7  1 17  7  3  3  2  9  1  6 16  1 11  4 16  2  1  7\n",
            "  5 12  1 12  2], shape=(101,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# Sanity check.\n",
        "for d in dataset.take(2):\n",
        "  print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEMn68-1avyq"
      },
      "source": [
        "The next step is to create batches from our dataset. To do this, we'll shuffle the dataset, then create batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G0Z0kLUgaq5M"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "batches = dataset.shuffle(10_000).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2-0vMuNa53Y",
        "outputId": "ba443393-e29d-4fa8-be62-6a024597e51b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 5 19 28 ... 19  3 10]\n",
            " [ 1 11  2 ... 22  4  9]\n",
            " [ 3  9  2 ... 10  6 20]\n",
            " ...\n",
            " [ 7  4  5 ... 35 34  1]\n",
            " [ 2  1  3 ...  4  5  1]\n",
            " [ 9  6 18 ... 15  2 11]], shape=(32, 101), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[ 8  3  9 ...  1 23  9]\n",
            " [ 1  5  6 ... 26  2  5]\n",
            " [19  3 10 ...  1 16  7]\n",
            " ...\n",
            " [ 9  3 13 ...  3 10  2]\n",
            " [ 1  3  6 ... 14 27 30]\n",
            " [ 2  1 17 ...  9  2 21]], shape=(32, 101), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for b in batches.take(2):\n",
        "  print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KcYdtzsbDPG"
      },
      "source": [
        "We can now separate each example into an input sequence(x) and a corresponding label/target sequence(y).<br><br>\n",
        "In the slides, we talked about **Teacher Forcing** where:<br>\n",
        "1. At each timestep during training, the output is compared to a label.\n",
        "2. At the next timestep, rather than feeding the model the previous output, we feed it the next character of the input sequence (i.e. what the model should've outputted).\n",
        "<br><br>\n",
        "\n",
        "So if a sequence is \"she swam in the lake\", then:\n",
        "- The input will be \"she swam in the lak\" (drop the last character)\n",
        "- The target/label will be \"he swam in the lake\" (drop the first character)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mI9rULq_a7iK"
      },
      "outputs": [],
      "source": [
        "xy_batches = batches.map(lambda batch: (batch[:, :-1], batch[:, 1:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyFFwopybXw4",
        "outputId": "a3965d6a-f1e2-4baf-8b2d-9e48d81921cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(32, 100), dtype=int32, numpy=\n",
            "array([[ 7,  3,  3, ...,  9,  6, 18],\n",
            "       [ 3,  2, 12, ..., 14,  3, 10],\n",
            "       [ 5, 12,  1, ...,  5, 19,  1],\n",
            "       ...,\n",
            "       [12,  1,  3, ...,  9,  7,  3],\n",
            "       [ 3, 10,  2, ..., 12,  2, 16],\n",
            "       [ 2, 12, 51, ...,  1,  6, 25]], dtype=int32)>, <tf.Tensor: shape=(32, 100), dtype=int32, numpy=\n",
            "array([[ 3,  3,  2, ...,  6, 18,  1],\n",
            "       [ 2, 12,  1, ...,  3, 10,  7],\n",
            "       [12,  1,  7, ..., 19,  1,  7],\n",
            "       ...,\n",
            "       [ 1,  3, 10, ...,  7,  3,  7],\n",
            "       [10,  2,  1, ...,  2, 16,  2],\n",
            "       [12, 51, 14, ...,  6, 25,  2]], dtype=int32)>)\n"
          ]
        }
      ],
      "source": [
        "for b in xy_batches.take(1):\n",
        "  print(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbPimYvtbYIo",
        "outputId": "50becc98-4b96-4ec6-b416-fbacf2886029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1 length:  100\n",
            "x1:  [23  2  1 12  7 17 22  2 12 21  1  4 16 14 18  6 13  1 11  7 18  1  8  4\n",
            "  2 19  2  1  3  6  1  7  1  3  6 20  5 24  1 18  6 13  1 20  4 11 11  1\n",
            "  2 40 10  7 13  8  3  1 18  6 13  9  1  8  3  9  2  5 19  3 10 21 14 14\n",
            " 30 21  1  7 19  7  4  5 24  1  4 16  1  3 10  2  1 15  7 17 22  7  4 19\n",
            "  5  1  4  8]\n",
            "\n",
            "\n",
            "y1 length:  100\n",
            "y1:  [ 2  1 12  7 17 22  2 12 21  1  4 16 14 18  6 13  1 11  7 18  1  8  4  2\n",
            " 19  2  1  3  6  1  7  1  3  6 20  5 24  1 18  6 13  1 20  4 11 11  1  2\n",
            " 40 10  7 13  8  3  1 18  6 13  9  1  8  3  9  2  5 19  3 10 21 14 14 30\n",
            " 21  1  7 19  7  4  5 24  1  4 16  1  3 10  2  1 15  7 17 22  7  4 19  5\n",
            "  1  4  8  1]\n"
          ]
        }
      ],
      "source": [
        "# For greater clarity, this is the first input sequence from the first batch,\n",
        "# and it's corresponding label/target sequence.\n",
        "for b in xy_batches.take(1):\n",
        "  print(\"x1 length: \", len(b[0][0].numpy()))\n",
        "  print(\"x1: \", b[0][0].numpy())\n",
        "  print(\"\\n\")\n",
        "  print(\"y1 length: \", len(b[1][0].numpy()))\n",
        "  print(\"y1: \", b[1][0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Yglbclb5zC"
      },
      "source": [
        "The last step before we can build our model is to one-hot encode the **inputs**. We're doing this because:\n",
        "1. We're not using embeddings for the input. We can, but since this is a character model with just a few dozen possible choices, we can get away with one-hot encoding. There's also no reason to think a particular letter should be closer to another in vector space as we would want in a word-level model.\n",
        "\n",
        "2. Since we're not using embeddings and our input is categorical, we need to one-hot encode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dY_O0kuBbwJG"
      },
      "outputs": [],
      "source": [
        "num_tokens = len(tokenizer.word_index) + 1\n",
        "\n",
        "# One-hot encode the input sequences, don't do anything with the label/target sequences.\n",
        "xy_batches = xy_batches.map(lambda inputs, labels: (tf.one_hot(inputs, depth=num_tokens), labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coq4b9HOcQJk",
        "outputId": "c305205e-f08b-4810-ee6e-db1b22584ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1:  [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "\n",
            "y1:  [ 8  1  3  6  1 23  7 13 11 26  1  3 10  2  1  2  5  2 17 18 36  8  1 22\n",
            " 11  7  5  8 28 14  3 10  2  1  5  2 40  3  1 23  2  8  3  1  4  8  1  3\n",
            "  6  1 22  9  2 25  2  5  3  1  3 10  2  1 46 13  5 15  3  4  6  5  1  6\n",
            " 16  1  3 10  2  1  2  5  2 17 18 36  8  1 16  6  9 15  2  8 28  1  3 10\n",
            "  2 14  5  2]\n"
          ]
        }
      ],
      "source": [
        "# Sanity check.\n",
        "for b in xy_batches.take(1):\n",
        "  print(\"x1: \", b[0][0].numpy())\n",
        "  print(\"\\n\")\n",
        "  print(\"y1: \", b[1][0].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5U0oJrK5cVMT"
      },
      "outputs": [],
      "source": [
        "# prefetching is an optimization step\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG7O09p2ckSJ"
      },
      "source": [
        "At this point, we've:\n",
        "- Segmented our corpus into fixed-length sequences.\n",
        "- Created training and label/target sequences.\n",
        "- Organized them into batches.\n",
        "- prepares the next batch (Prefetch dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_Fv1ImFcxcP"
      },
      "source": [
        "# Stacked LSTMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWXutnw_cVJa",
        "outputId": "93b8157c-53e4-41af-93ef-902e395c51c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, None, 128)         95232     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 128)         0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, None, 128)         131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 57)          7353      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 234,169\n",
            "Trainable params: 234,169\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# recurrent_dropout=0 to support cuDNN \n",
        "model = Sequential([\n",
        "    LSTM(128, return_sequences=True, input_shape=[None, num_tokens], recurrent_dropout=0),\n",
        "    Dropout(0.3),\n",
        "    LSTM(128, return_sequences=True, recurrent_dropout=0),\n",
        "    Dense(num_tokens, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam')\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDSL3yHcdEIY",
        "outputId": "65b8b201-292a-41c6-8979-85577270e445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1905/1905 [==============================] - 33s 14ms/step - loss: 2.1523\n",
            "Epoch 2/50\n",
            "1905/1905 [==============================] - 28s 14ms/step - loss: 1.5404\n",
            "Epoch 3/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 1.2243\n",
            "Epoch 4/50\n",
            "1905/1905 [==============================] - 28s 14ms/step - loss: 1.0091\n",
            "Epoch 5/50\n",
            "1905/1905 [==============================] - 29s 14ms/step - loss: 0.8670\n",
            "Epoch 6/50\n",
            "1905/1905 [==============================] - 27s 14ms/step - loss: 0.7676\n",
            "Epoch 7/50\n",
            "1905/1905 [==============================] - 28s 14ms/step - loss: 0.6962\n",
            "Epoch 8/50\n",
            "1905/1905 [==============================] - 28s 13ms/step - loss: 0.6415\n",
            "Epoch 9/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.5989\n",
            "Epoch 10/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.5650\n",
            "Epoch 11/50\n",
            "1905/1905 [==============================] - 28s 13ms/step - loss: 0.5372\n",
            "Epoch 12/50\n",
            "1905/1905 [==============================] - 28s 14ms/step - loss: 0.5139\n",
            "Epoch 13/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.4934\n",
            "Epoch 14/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.4756\n",
            "Epoch 15/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.4611\n",
            "Epoch 16/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.4474\n",
            "Epoch 17/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.4362\n",
            "Epoch 18/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.4257\n",
            "Epoch 19/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.4163\n",
            "Epoch 20/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.4081\n",
            "Epoch 21/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.4000\n",
            "Epoch 22/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3938\n",
            "Epoch 23/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3873\n",
            "Epoch 24/50\n",
            "1905/1905 [==============================] - 29s 14ms/step - loss: 0.3811\n",
            "Epoch 25/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3755\n",
            "Epoch 26/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3716\n",
            "Epoch 27/50\n",
            "1905/1905 [==============================] - 29s 14ms/step - loss: 0.3663\n",
            "Epoch 28/50\n",
            "1905/1905 [==============================] - 27s 14ms/step - loss: 0.3625\n",
            "Epoch 29/50\n",
            "1905/1905 [==============================] - 28s 14ms/step - loss: 0.3579\n",
            "Epoch 30/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3539\n",
            "Epoch 31/50\n",
            "1905/1905 [==============================] - 28s 14ms/step - loss: 0.3515\n",
            "Epoch 32/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3476\n",
            "Epoch 33/50\n",
            "1905/1905 [==============================] - 28s 14ms/step - loss: 0.3443\n",
            "Epoch 34/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3411\n",
            "Epoch 35/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3390\n",
            "Epoch 36/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3355\n",
            "Epoch 37/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3334\n",
            "Epoch 38/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3306\n",
            "Epoch 39/50\n",
            "1905/1905 [==============================] - 29s 14ms/step - loss: 0.3285\n",
            "Epoch 40/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3262\n",
            "Epoch 41/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3236\n",
            "Epoch 42/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3216\n",
            "Epoch 43/50\n",
            "1905/1905 [==============================] - 30s 14ms/step - loss: 0.3196\n",
            "Epoch 44/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3177\n",
            "Epoch 45/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3156\n",
            "Epoch 46/50\n",
            "1905/1905 [==============================] - 29s 14ms/step - loss: 0.3140\n",
            "Epoch 47/50\n",
            "1905/1905 [==============================] - 28s 14ms/step - loss: 0.3117\n",
            "Epoch 48/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3106\n",
            "Epoch 49/50\n",
            "1905/1905 [==============================] - 27s 13ms/step - loss: 0.3088\n",
            "Epoch 50/50\n",
            "1905/1905 [==============================] - 28s 13ms/step - loss: 0.3072\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(xy_batches, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTLAzuFFeDMa",
        "outputId": "db91f560-6257-4a2d-89d2-fe577988ab83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "# save model\n",
        "model.save('art_of_war_char_level_lm_50epochs')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Text\n",
        "\n",
        "There's a *temperature* parameter. The next character is picked from a probability distribution. By dividing the log of this distribution by *temperature*, we can influence the randomness of the output.<br><br>\n",
        "When the temperature is low (< 1), the probability distribution sharpens and the model will be more strict in recreating the original text. As we raise the temperature, the distribution flattens and there's a higher chance the model picks something unexpected, resulting in greater surprise in the output. In practice, a high enough temperature will result in nonsense."
      ],
      "metadata": {
        "id": "wF4cyMwZp0I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "def generate_text(seed_text, length=200, temperature=1):\n",
        "\n",
        "  text = seed_text  \n",
        "\n",
        "  for _ in range(length):\n",
        "\n",
        "    # Take the last *input_timesteps* number of characters in the text so far\n",
        "    # as input.\n",
        "    input = np.array(tokenizer.texts_to_sequences([text[-input_timesteps:]]))\n",
        "    input = tf.one_hot(input, num_tokens)\n",
        "\n",
        "    # Create probability distribution for next character adjusted by temperature.\n",
        "    preds = model.predict(input, verbose=0)[0, -1:, :] # <-- We want only the last character so we're extracting the softmax output for that.\n",
        "    preds = tf.math.log(preds) / temperature\n",
        "\n",
        "    # Sample next character and add to running text.\n",
        "    next_char = tf.random.categorical(preds, num_samples=1)\n",
        "    next_char = tokenizer.sequences_to_texts(next_char.numpy())[0]\n",
        "\n",
        "    # GPT-3 like prompt :D\n",
        "    sys.stdout.write(next_char)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    text += next_char\n",
        "  print()\n",
        "    \n",
        "  return text"
      ],
      "metadata": {
        "id": "n2_S7l3ep4jE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Banana peels on the battlefield can \", length=30, temperature=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DjVcoVrsp9cx",
        "outputId": "5a88e441-e51d-4d36-8008-9580a4cf4336"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "on hemments, and when the met \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Banana peels on the battlefield can on hemments, and when the met '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"It's time to release the Kraken when \", length=30, temperature=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "t7ljz6hRqFLe",
        "outputId": "0bad61fb-6c3c-4cfb-fc73-a3b33ad4e5fd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keves under comit. sollies wit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It's time to release the Kraken when keves under comit. sollies wit\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"Crush your enemies, see them driven before you, and \", length=30, temperature=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "13skICO6qGPC",
        "outputId": "27eb8b84-6bd6-4402-dc76-84b430265173"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for the road.\n",
            "\n",
            "50. foo even th\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Crush your enemies, see them driven before you, and for the road.\\n\\n50. foo even th'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(\"What is best in life? \", length=30, temperature=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "8P27Vm9lqHOD",
        "outputId": "50c677d2-6771-428e-9eb4-631bf79bdf0c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in\n",
            "simply because or\n",
            " appearan\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is best in life? in\\nsimply because or\\n appearan'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few observations of the preceding outputs:\n",
        "1. Despite being a character-level model, the model managed to \"learn\" spelling, cadence, punctuation, spacing, grammar, and even numbered bullet points just from trying to predict the next character.\n",
        "\n",
        "2. It's pretty cool how the model manages to take our initial seed text and complete a sentence with it before moving on.\n",
        "\n",
        "3. We can see the output getting increasingly nonsensical as the temperature rises. What temperature to use ultimately depends on the nature of your corpus and your goals with the language model.\n",
        "\n",
        "Also, in contrast to our language model, GPT-3 has 175 billion parameters and was trained on 45 terabytes of data, but the high-level principle of learning through prediction remains the same."
      ],
      "metadata": {
        "id": "vcqOLdp2qJ4M"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}